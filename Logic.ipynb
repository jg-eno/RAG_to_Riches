{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.49 (from langchain)\n",
      "  Downloading langchain_core-0.3.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.24-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.2-py3-none-any.whl.metadata (64 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.49->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.49->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.1)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading langchain-0.3.22-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_core-0.3.50-py3-none-any.whl (423 kB)\n",
      "Using cached langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.24-py3-none-any.whl (352 kB)\n",
      "Downloading pydantic-2.11.2-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-inspection, tenacity, sniffio, PyYAML, pydantic-core, orjson, jsonpointer, idna, h11, greenlet, charset-normalizer, certifi, async-timeout, annotated-types, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, anyio, requests-toolbelt, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.40 annotated-types-0.7.0 anyio-4.9.0 async-timeout-4.0.3 certifi-2025.1.31 charset-normalizer-3.4.1 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.22 langchain-core-0.3.50 langchain-text-splitters-0.3.7 langsmith-0.3.24 orjson-3.10.16 pydantic-2.11.2 pydantic-core-2.33.1 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-inspection-0.4.0 urllib3-2.3.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
      "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.49 in ./.venv/lib/python3.10/site-packages (from langchain-google-genai) (0.3.50)\n",
      "Requirement already satisfied: pydantic<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain-google-genai) (2.11.2)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.3.24)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.13.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in ./.venv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.23.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.14.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.3.1)\n",
      "Downloading langchain_google_genai-2.1.2-py3-none-any.whl (42 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: filetype, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-api-core, google-ai-generativelanguage, langchain-google-genai\n",
      "Successfully installed cachetools-5.5.2 filetype-1.2.0 google-ai-generativelanguage-0.6.17 google-api-core-2.24.2 google-auth-2.38.0 googleapis-common-protos-1.69.2 grpcio-1.71.0 grpcio-status-1.71.0 langchain-google-genai-2.1.2 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "API_KEY = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-2.5-pro-exp-03-25',api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a large language model, trained by Google.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Who are you ? \").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_chroma\n",
      "  Using cached langchain_chroma-0.2.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain_text_splitters in ./.venv/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in ./.venv/lib/python3.10/site-packages (from langchain_chroma) (0.3.50)\n",
      "Collecting numpy<2.0.0,>=1.22.4 (from langchain_chroma)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain_chroma)\n",
      "  Using cached chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in ./.venv/lib/python3.10/site-packages (from langchain_community) (0.3.22)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain_community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain_community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain_community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.10/site-packages (from langchain_community) (0.3.24)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading multidict-6.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.venv/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.11.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.13.1)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading onnxruntime-1.21.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting tqdm>=4.65.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting overrides>=7.3.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (24.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.38.0)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.29.4)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.69.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./.venv/lib/python3.10/site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
      "Using cached langchain_chroma-0.2.2-py3-none-any.whl (11 kB)\n",
      "Using cached langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "Using cached chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Using cached kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading multidict-6.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading onnxruntime-1.21.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\n",
      "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Downloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: pypika, mpmath, monotonic, flatbuffers, durationpy, zipp, wrapt, websockets, websocket-client, uvloop, tqdm, tomli, sympy, shellingham, pyproject_hooks, propcache, overrides, opentelemetry-util-http, opentelemetry-proto, oauthlib, numpy, mypy-extensions, multidict, mmh3, mdurl, marshmallow, importlib-resources, humanfriendly, httpx-sse, httptools, fsspec, frozenlist, filelock, distro, click, bcrypt, backoff, attrs, asgiref, aiohappyeyeballs, yarl, watchfiles, uvicorn, typing-inspect, starlette, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, markdown-it-py, importlib-metadata, huggingface-hub, deprecated, coloredlogs, chroma-hnswlib, build, aiosignal, tokenizers, rich, pydantic-settings, opentelemetry-api, onnxruntime, kubernetes, fastapi, dataclasses-json, aiohttp, typer, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, langchain_community, chromadb, langchain_chroma\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 asgiref-3.8.1 attrs-25.3.0 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 click-8.1.8 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.18 distro-1.9.0 durationpy-0.9 fastapi-0.115.12 filelock-3.18.0 flatbuffers-25.2.10 frozenlist-1.5.0 fsspec-2025.3.2 httptools-0.6.4 httpx-sse-0.4.0 huggingface-hub-0.30.1 humanfriendly-10.0 importlib-metadata-8.6.1 importlib-resources-6.5.2 kubernetes-32.0.1 langchain_chroma-0.2.2 langchain_community-0.3.20 markdown-it-py-3.0.0 marshmallow-3.26.1 mdurl-0.1.2 mmh3-5.1.0 monotonic-1.6 mpmath-1.3.0 multidict-6.3.2 mypy-extensions-1.0.0 numpy-1.26.4 oauthlib-3.2.2 onnxruntime-1.21.0 opentelemetry-api-1.31.1 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 propcache-0.3.1 pydantic-settings-2.8.1 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 rich-14.0.0 shellingham-1.5.4 starlette-0.46.1 sympy-1.13.3 tokenizers-0.21.1 tomli-2.2.1 tqdm-4.67.1 typer-0.15.2 typing-inspect-0.9.0 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4 websocket-client-1.8.0 websockets-15.0.1 wrapt-1.17.2 yarl-1.18.3 zipp-3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_chroma langchain_community langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_core in ./.venv/lib/python3.10/site-packages (0.3.50)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.10/site-packages (from langchain_core) (0.3.24)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain_core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain_core) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.10/site-packages (from langchain_core) (4.13.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.venv/lib/python3.10/site-packages (from langchain_core) (2.11.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.4.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain_core) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain_core) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydantic in ./.venv/lib/python3.10/site-packages (2.11.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./.venv/lib/python3.10/site-packages (from pydantic) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.10/site-packages (from pydantic) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic) (0.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_core\n",
    "%pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(path):\n",
    "    doc_loader = PyPDFLoader(path)\n",
    "    print(doc_loader.load())\n",
    "    return doc_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./.venv/lib/python3.10/site-packages (from pypdf) (4.13.1)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4_VS_Human_translators.pdf\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='GPT-4 vs. Human Translators: A Comprehensive Evaluation of\\nTranslation Quality Across Languages, Domains, and Expertise Levels\\nJianhao Yan1,2∗ Pingchuan Yan3∗ Yulong Chen4∗\\nJudy Li5 Xianchao Zhu5 Yue Zhang2,6,\\x00\\n1 Zhejiang University 2 School of Engineering, Westlake University\\n3 University College London 4 University of Cambridge 5 Lan-Bridge Group\\n6 Institute of Advanced Technology, Westlake Institute for Advanced Study\\nelliottyan37@gmail.com\\nAbstract\\nThis study comprehensively evaluates the\\ntranslation quality of Large Language Mod-\\nels (LLMs), specifically GPT-4, against hu-\\nman translators of varying expertise lev-\\nels across multiple language pairs and do-\\nmains. Through carefully designed annota-\\ntion rounds, we find that GPT-4 performs\\ncomparably to junior translators in terms of\\ntotal errors made but lags behind medium\\nand senior translators. We also observe the\\nimbalanced performance across different lan-\\nguages and domains, with GPT-4’s transla-\\ntion capability gradually weakening from\\nresource-rich to resource-poor directions. In\\naddition, we qualitatively study the trans-\\nlation given by GPT-4 and human transla-\\ntors, and find that GPT-4 translator suffers\\nfrom literal translations, but human trans-\\nlators sometimes overthink the background\\ninformation. To our knowledge, this study\\nis the first to evaluate LLMs against human\\ntranslators and analyze the systematic differ-\\nences between their outputs, providing valu-\\nable insights into the current state of LLM-\\nbased translation and its potential limitations.\\n1 Introduction\\nRecent studies show that LLMs can serve as a\\nstrong translation system and a good substitute for\\nNMT models (Jiao et al., 2023a; Wang et al., 2023a;\\nEnis and Hopkins, 2024; Huang et al., 2023; Wu\\net al., 2024a; Hendy et al., 2023; Peng et al., 2023).\\nFor example, Jiao et al. (2023a) and Wang et al.\\n(2023a) find that GPT-4 can outperform commer-\\ncial machine translation systems via automatic and\\nhuman evaluations. Such impressive results have\\nhastened a wide range of applications, including\\nthe use of GPT-4 for literary translation (Wu et al.,\\n2024b).\\n∗These authors contributed equally to this work.\\nDespite their impressive capabilities, the nature\\nof LLM output compared to human translators\\nremains unclear. This raises two critical ques-\\ntions: (1) How do LLMs compare to human ex-\\nperts in translation quality? and (2) Are there\\nfundamental differences in their outputs? These\\ninquiries are particularly relevant in light of recent\\nresearch demonstrating significant distinctions be-\\ntween LLM-generated and human-generated texts\\nin general (Li et al., 2023; Bao et al., 2023). Such\\nfindings suggest that even if LLMs produce high-\\nquality translations, their outputs may possess\\nunique characteristics that distinguish them from\\nhuman-produced translations.\\nTo determine where LLMs fall within the spec-\\ntrum of human translation proficiency, which\\nranges from novice translators to seasoned profes-\\nsionals, we study the problem by taking the current\\nrepresentative LLM, i.e., GPT-4, and comparing\\nit against human translators with different exper-\\ntise. We first conduct a preliminary study compar-\\ning human translations against GPT-4 translations,\\nfinding that even experts cannot reach a consen-\\nsus on which translation is better . Given these\\nfindings, we take a finer-grained evaluation across\\ndifferent languages and domains, so that translation\\nquality can be better calibrated and systematic dif-\\nferences can be measured. Our evaluation covers\\nthree language pairs from resource-rich to resource-\\npoor, i.e., Chinese ↔English, Russian ↔English,\\nand Chinese↔Hindi, and three domains, i.e., News,\\nTechnology, and Biomedical. Given a source sen-\\ntence, we ask junior, medium, and senior trans-\\nlators and GPT-4 to generate the corresponding\\ntranslation in the target language. Then given each\\ntranslation pair, we hire independent expert annota-\\ntors to label the errors in the target sentence under\\nthe MQM schema (Freitag et al., 2021). We find\\nthat GPT-4 reaches a comparable performance to\\njunior translators in the perspective of total errors\\nmade, and lags behind senior ones with a consider-\\narXiv:2407.03658v1  [cs.CL]  4 Jul 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='able gap.\\nOur further analyses and qualitative studies show\\nthat there are imbalanced performances for differ-\\nent languages and domains. From resource-rich to\\nresource-poor directions, GPT-4’s translation capa-\\nbility gradually weakens. For resource-rich direc-\\ntions like Chinese↔English, GPT-4 performs com-\\nparably with junior translators and even close to\\nmedium translators, but in Chinese↔Hindi, it even\\nlags behind our baseline system. The weaknesses\\nmentioned above are also general shortcomings of\\nlarge models and reflect that although large models\\nhave achieved universal translation with a focus\\non one language, translation between low-resource\\nlanguages remains a relative weakness.\\nTo our knowledge, we are the first to evaluate\\nLLMs against human translators and analyze the\\nsystematic differences between LLMs and human\\ntranslators.\\n2 Related Work\\nBenchmarking LLMs Previous studies have\\nbenchmarked LLMs on various NLP tasks. Xu et al.\\n(2020) benchmark several LLMs on Chinese text,\\nevaluating their Chinese ability. Ye et al. (2024)\\nassess LLMs through Question Answering (QA),\\nMMLU (Hendrycks et al., 2021), and other metrics.\\nFrom these tests, LLMs with larger scales are gen-\\nerally proved to be more accurate except for certain\\ntasks. Yuan et al. (2023) demonstrates that LLMs\\nperform well in long-context understanding and\\nare more capable with Out-of-Distribution, which\\nmeans LLMs have a certain degree of generaliza-\\ntion ability.\\nFurther to the MT field, Jiao et al. (2023b) find\\nthat GPT-4 performed competitively with other\\nSotA translation products. Wang et al. (2023a)\\nfurther investigated the capability of GPT-4 in\\ndocument-level translation, the results show that\\nGPT-4 performs better than commercial translation\\nproducts and document NMT methods. Compared\\nto them, our work empirically shows that GPT-4 is\\ncomparable to junior human translators.\\nLLMs as Human ExpertsDue to the great ca-\\npacities of GPT-4 over traditional NLP models,\\nresearchers have investigated and compared the\\nperformance of GPT-4 as human experts in mul-\\ntiple NLP tasks. Zhu et al. (2024) highlight that\\nGPT-4 and GPT-4-turbo show top performance on\\na Chinese financial language understanding task.\\nLiu et al. (2023b) find the LLMs can be benefi-\\ncial to biomedical NLP tasks. Goyal et al. (2022)\\ncompare GPT models with several summarization\\nmodels and humans, and find that GPT can gen-\\nerate summaries preferred by humans. In AI for\\neducation area, Nguyen and Allan (2024) show\\nGPT-4’s can provide teaching feedback for stu-\\ndents. Maloney et al. (2024) find that GPT-4 shows\\nclose performance compared with human partici-\\npants in coordination games. Siu (2023) show that\\nGPT-4 is comparable to humans on technical trans-\\nlation tasks. Bojic et al. (2023) find that GPT-4 can\\noutperform human experts on linguistic pragmatic\\ntasks. In clinical diagnostics, Han et al. (2023)\\nfind that GPT-4 can give comparable performance\\nto humans, and GPT-4v (vision version) can even\\noutperform human experts.\\nHuman Evaluation for MT (Graham et al.,\\n2013) first propose Direct Assessment (DA), which\\nuses a continuous score from 0 to 100 to repre-\\nsent the quality of a hypothesis. DA has been\\nadopted in WMT translation tasks for the past\\nfew years (Farhad et al., 2021; Kocmi et al., 2022,\\n2023). MQM (Lommel et al., 2014), the annota-\\ntion used in this paper, is another widely used an-\\nnotation scheme (Klubiˇcka et al., 2018; Rei et al.,\\n2020a). It requires the annotators to annotate the\\nerror span for each hypothesis and is shown to be\\nmore accurate and reliable than DA (Freitag et al.,\\n2021). Thus, it is utilized in the metrics tasks of\\n2022 and 2023 WMT challenges (Freitag et al.,\\n2022, 2023).\\nHuman Parity The human parity for machine\\ntranslation systems is first claimed by (Hassan et al.,\\n2018), which describes a comparable performance\\non the WMT 2017 news translation task from Chi-\\nnese to English when compared to professional hu-\\nman translations. However, this claim is challenged\\nby the following research, raising concerns about\\nthe limited scope of human parity. These limita-\\ntions include the expertise of human evaluators (Fis-\\ncher and L¨\"aubli, 2020), the origin and quality of\\nsource sentences (Toral et al., 2018; Kim et al.,\\n2023), the limited scenario of comparison (Poibeau,\\n2022) and difficulty of translation (Graham et al.,\\n2020), indicating significant gaps between NMT\\nmodels and the professional translators. In this\\nwork, we evaluate whether the SOTA LLM GPT-\\n4 performs comparable to professional translators\\nand what differs between human translators and\\nLLMs. With the above lessons in mind, we address'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='these limitations by hiring expert annotators, avoid-\\ning target-origin source text, manually evaluating\\nsource sentences, and covering high-resource to\\nlow-resource language pairs and various domains.\\n3 Preliminary Study\\nThis section presents our preliminary study. We\\naim to first compare GPT-4 translations with hu-\\nman translations qualitatively, in a coarse manner.\\nOur comparison is simple and direct. We sample\\nhuman-translated texts and prompt GPT-4 to trans-\\nlate the same source sentence. Then, we ask expert\\nannotators to determine which translation is better.\\nParticularly, to have a quick overview of the qual-\\nities of human translations against GPT-4 transla-\\ntions, we first utilize COMET-QE 1 to score our\\nin-house Chinese to English human-translated doc-\\numents, and select two documents with the highest\\nscore and the lowest score. Note that our in-house\\ntranslated documents are all translated by profes-\\nsional translators. In this way, we gather 40 pairs\\nof translations from professional translators and\\nGPT-4, respectively. Recent findings (Freitag et al.,\\n2021) have demonstrated that crowd-sourced hu-\\nman ratings are less reliable for high-quality MT\\nevaluation. Thus, we hire six expert annotators to\\ncompare the two translations and select the better\\ntranslations they find. We randomly shuffle the\\nGPT-4 and human translations to prevent annota-\\ntors from identifying GPT-4.\\nThe average win rate of GPT is\\n15.5/40 (36.25%). It looks like a clear win\\nfor human translators, but when delving deeper,\\nwe find that the expert annotators have a low\\nratio of agreement with each other. In Table 1,\\nmost annotators only agree with each other at\\naround 60% (the baseline is 50%) of an agreed\\nwinner at each source sentence. We further\\nconduct a significance test and only annotator\\nB finds human translation significantly better\\nthan GPT’s translation and other annotators\\nhave high p-values. Given annotators’ expertise\\nand our task is straightforward, these results\\nindicate that even expert annotators find it difficult\\nto agree on which translation is better , and\\nGPT-generated translations might have different\\nadvantages against human-generated ones. These\\nresults motivate us to conduct a finer-grained and\\ncomprehensive evaluation to reveal the systematic\\ndifference between GPT-4 and human translations.\\n1Unbabel/wmt23-cometkiwi-da-xl\\nAnnotators A B C D E F\\nA 100.0 57.5 65.0 65.0 62.5 67.5\\nB - 100.0 52.5 52.5 50.0 50.0\\nC - - 100.0 65.0 82.5 67.5\\nD - - - 100.0 57.5 62.5\\nE - - - - 100.0 70.0\\nF - - - - - 100.0\\np-value 1.000 0.038 0.268 0.081 0.154 0.875\\nTable 1: Ratio(%) of agreed winner across expert\\nannotators and significance p-value for binomial\\ntest. P-value < 0.05 denotes a significant difference\\nbetween GPT-4 and Human.\\n4 Main Experimental Setup\\nMotivated by the results from our preliminary\\nstudy, we conduct a comprehensive and fine-\\ngrained evaluation, for revealing the systematic\\ndifference between humans and GPTs. Specifically,\\nwe employed the widely recognized Multidimen-\\nsional Quality Metrics (MQM) framework (Lom-\\nmel et al., 2014) and compared human translators\\nwith varying levels of expertise to GPT-4. Our\\nevaluation spans multiple languages and domains,\\naiming to furnish broad insights into these compar-\\nisons.\\n4.1 Data Collection\\nWe collect multilingual and multi-domain source\\nsentences. Our multilingual evaluation data con-\\ntains six language directions, covering high re-\\nsource to low resource, including English to Chi-\\nnese, Chinese to English, English to Russian, Rus-\\nsian to English, English to Hindi, and Hindi to\\nEnglish.\\nFor general domain Chinese ⇔English and\\nEnglish⇔Russian, we sample source sentences\\nfrom test sets of WMT2023 and WMT2022, re-\\nspectively. For Chinese⇔Hindi, we extract source\\nnews text from public websites. For multi-domain\\nevaluation data, we evaluate two domains, i.e.,\\nbiomedical and technology and we evaluate Chi-\\nnese to English. The source sentences are extracted\\nnews texts from public websites. We ensure that\\nall sources are source language origin to avoid the\\neffect of translationese. We manually evaluate all\\nsource sentences for these tasks and ensure the\\nsource sentences are not too easy or too short. Fi-\\nnally, each task contains 200 sentences, making our\\nevaluation a total of 1600 sentences.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='Type Error Name Explanations\\nAccuracy\\nMistranslation Translation does not accurately represent the source.\\nAddition Information not present in the source.\\nMT Hallucination Information that has nothing related to source; or gibber-\\nish; or repeats\\nOmission Missing content from the source.\\nUntranslated Not translated.\\nWrong Name Entity and Term Wrong usage of NE and Terminology.\\nFluency\\nGrammar Problems with grammar of target language.\\nPunctuation Incorrect punctuation (for locale or style)\\nSpelling Incorrect spelling or capitalization.\\nRegister Wrong grammatical register (e.g., inappropriately infor-\\nmal pronouns).\\nInconsistent Style Internal inconsistency ( not related to terminology )\\nUnnatural Flow Translations that are too literal or sound unnatural.\\nOther Non-translation -\\nTable 2: Error category and explanations. We mainly follow the guidelines from Unbabel, and merge some\\nerrors to reduce the efforts for annotators to understand the annotation system. Concrete examples for\\neach error category can be found in the Appendix.\\n4.2 Human Translators and Machine\\nTranslators\\nWe ask different human translators to translate our\\nsource sentences into the target language. Transla-\\ntors are of three different levels of expertise, cate-\\ngorized into junior-level, medium-level, and senior-\\nlevel translators. The level of expertise is ranked\\nby in-house criteria covering the translators’ edu-\\ncational background, translation experience, and\\npractical proficiency. See Appendix A for more de-\\ntails. For a fair comparison, we request the experts\\nnot use machine translation or GPTs as assistance.\\nFor all directions except Zh-Hi and Hi-Zh, we col-\\nlect three human translation results from each level\\nof expertise. For Zh-Hi and Hi-Zh, we only have\\nmedium-level and senior-level translators due to\\nthe scarcity of translators.\\nExcept for human translators, we use\\ngpt-4-1106-preview, the current state-\\nof-the-art large language model released by\\nOpenAI and Seamless M4T (Communication\\net al., 2023) as the representative of traditional ma-\\nchine translations to complement our experiments.\\nWe directly prompt GPT-4 to obtain the translation,\\nas it is the most common practice for normal users,\\nthe easiest to reproduce, and to avoid confusion by\\nvarious techniques.\\n4.3 Prompt Search\\nPrevious study (Zhao et al., 2021; Liu et al., 2023a)\\nshows that different prompts with LLMs can result\\nin distinctive performance. Thus, we collect three\\ncandidate prompts used in previous research (Xu\\net al., 2023; Jiao et al., 2023a) and use COMET-\\nQE (Rei et al., 2020b) to select the best prompt\\nto make the best use of GPT-4, as shown in Table\\n3. In particular, we use these three prompts to\\nprompt GPT-4 to translate 100 source sentences in\\nour Chinese-to-English test set and adopt COMET-\\nQE to evaluate the quality of translations. We find\\nthat the third prompt yields the best performance,\\nand hence we adopt this prompt for all following\\nexperiments.\\n4.4 Annotation Protocol\\nTo evaluate the results of candidates’ systems,\\nwe hire experts to annotate the errors of trans-\\nlations blindly. The annotation platform is Doc-\\ncano (Nakayama et al., 2018), and the error tags\\nare made according to MQM standards. MQM\\nrequires the annotators to annotate the span of er-\\nrors in each hypothesis. All hypotheses of the same\\nsource sentence are shown to the annotator together\\nto help decide which is better. We have 13 error\\ncategories and two severities, as shown in Table\\n2. Our categorization for errors mostly follows'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='Prompt COMET\\nPlease translate the following sen-\\ntence from Chinese into English.\\nYour language and style should align\\nwith the language conventions of a\\nnative speaker. \\\\n{SOURCE}\\\\n\\n0.775\\nYou are an expert translator for trans-\\nlating Chinese to English. Your\\nlanguage and style should align\\nwith the language conventions of\\na native speaker. \\\\n[Chinese]:\\n{SOURCE}\\\\n[English]:\\n0.755\\nPlease provide the English transla-\\ntion for these sentences. Your lan-\\nguage and style should align with\\nthe language conventions of a native\\nspeaker. \\\\n{SOURCE}\\\\n\\n0.780\\nTable 3: Taking Chinese to English as an example,\\nour three prompts and corresponding scores with\\nCOMET-QE.{SOURCE} represents the source sen-\\ntence to be translated.\\nUnbabel’s practice 2 and we focus on most com-\\nmon error types. Each tag has subtags with two\\nseverities, i.e., Minor or Major. A screenshot of the\\nannotation system is given in Figure 5.\\nFor each task, we first ask the two expert anno-\\ntators to carefully read our manual and conduct\\na training round on the first 10 groups of transla-\\ntions. Then, we manually check these annotations\\nto provide feedback and ask the two annotators to\\ncheck their disagreements and revise their results.\\nAfter two rounds of such training processes, we\\nask the annotators to finish the remaining sentences\\nwithout knowing each other’s results.\\nAfter the first round of annotation, we conduct a\\nsecond round to further refine the evaluation results.\\nIn particular, we hire another two experts for each\\ntask and show them the previous annotation results.\\nThey are asked to approve and make necessary\\nmodifications to previous round annotations.\\n4.5 Inter-Annotator Agreement\\nError annotation with MQM is challenging, and\\nprevious work demonstrates that the agreement\\nscores between MQM annotations are relatively\\nlow (Lommel et al., 2014). Reasons for this could\\nbe disagreement on precise spans and ambiguous\\n2https://help.unbabel.\\ncom/hc/en-us/articles/\\n6444304419479-Annotation-Guidelines-Typology-3-0\\nTask Cohen Kappa(Segment)Krippendorffs(Span)\\nReference, Re-Annotated by (Freitag et al., 2021)\\nWMT 2020 En-De 0.208 0.456\\nWMT 2021 En-De 0.230 0.501\\nOurs\\nGeneral Zh-En 0.257 0.436\\nGeneral En-Zh 0.544 0.579\\nGeneral En-Ru 0.461 0.566\\nGeneral Ru-En 0.341 0.875\\nGeneral Zh-Hi 0.256 0.443\\nGeneral Hi-Zh 0.234 0.495\\nTechnology Zh-En 0.306 0.581\\nBiomedical Zh-En 0.373 0.616\\nAverage 0.321 0.555\\nTable 4: Cohen Kappa (segment-level) and Krip-\\npendorffs’ Alpha (span-level) agreement of our an-\\nnotations.\\nerror categorization (Lommel et al., 2014). Despite\\nthe low agreement scores, MQM is more reliable\\nthan other evaluation protocols like Direct Assess-\\nment (Freitag et al., 2021).\\nTo compute inter-annotator agreement for MQM,\\nwe employ segment-level Cohen’s Kappa (Cohen,\\n1960) and span-level Krippendorff’s alpha (Krip-\\npendorff, 1980). For reference, we calculate the\\nagreement on the annotated results of the 2020 and\\n2021 WMT English-to-German tasks by (Freitag\\net al., 2021). Our IAA results are shown in Table 4.\\nThanks to our two-round annotation process, our\\nIAA scores show a favorable agreement, indicating\\na good annotation quality.\\n5 Main Results\\n5.1 Overall Results\\nAnalysis of Error Severity The upper part of\\nFigure 1 plots the averaged number of errors of\\ndifferent systems and translators. Compared to our\\nMT baseline (seamless), GPT-4 has much fewer er-\\nrors. It performs almost as well as the junior-level\\ntranslator at the level of total errors, as GPT-4 is\\nannotated with only slightly more minor and major\\nerrors than junior translators. However, GPT-4 still\\nhas clear performance gaps between medium or\\nsenior human translators, as it makes considerably\\nmore mistakes than experienced translators. To our\\nknowledge, we are the first to report how GPT-4 is\\non translation against human translators.\\nAnalysis of Error CategoriesFurthermore, we\\nplot the categories of errors in the bottom part of\\nFigure 1. Compared with junior human translators,\\nGPT-4 makes more errors in the accuracy of trans-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='seamless gpt4 junior medium senior\\nSystems\\n0\\n50\\n100\\n150\\n200Number of Errors\\nAverage Severity for Each System\\nMinor\\nMajor\\nseamless gpt4 junior medium senior\\nSystems\\n0\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200Number of Errors\\nError Category for Each System\\nAccuracy\\nFluency\\nFigure 1: Upper: Error severity for each system.\\nThe gray line represents the standard deviation for\\neach system across tasks. Bottom: Error category\\nanalysis for each system.\\nlations, which accounts for most of the disparity.\\nInterestingly, GPT-4 surpasses junior translators\\nin fluency issues, denoting a better capability of\\nlanguage usage.\\nIn addition, Figure 2 shows the top 5 categories\\nof errors made by different systems. ‘Mistransla-\\ntion’ is the most frequent error made by all systems.\\nImproving much over the seamless baseline, GPT-\\n4 makes comparable numbers of ‘Mistranslation’\\nwith junior and medium human translators.\\nFor all translators, ‘Unnatural Flow’ is among\\nthe most frequent errors. Seamless, GPT-4, and\\njunior translators have similar levels of ‘Unnatural\\nFlow’, indicating possible issues of literal transla-\\ntion and not following language conventions. In\\ncontrast, medium and senior translators are anno-\\ntated with significantly fewer errors of ‘Unnatural\\nFlow’.\\nIn addition, we notice even though GPT-4 makes\\nMistranslation\\nPunctuation\\nWrong NE\\nUnnatural Flow\\nOmission\\nMistranslationUnnatural Flow\\nWrong NEGrammar\\nPunctuation\\nMistranslationUnnatural Flow\\nGrammarWrong NE\\nPunctuation\\nMistranslationUnnatural Flow\\nWrong NEOmissionGrammar\\nMistranslationUnnatural Flow\\nAddition\\nPunctuation\\nGrammar\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80Number of Errors\\nT op5 Error Categories\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\nFigure 2: Top 5 categories of errors made by each\\nsystem.\\nmuch fewer ‘Wrong Name Entity(NE)’ errors com-\\npared to Seamless, which could be beneficial be-\\ncause of its huge knowledge acquired in the pre-\\ntraining stage, it still has a gap compared to human\\ntranslators.\\nFinally, we notice that GPT-4 does not have\\nOmission or Addition problems in its top-5 errors,\\nwhereas even senior translators have Addition er-\\nrors.\\n5.2 Detailed Results for Each Language\\nIn Figure 3, we present detailed results for each\\nlanguage pair, averaged over two directions.\\nEnglish-Chinese From Figure 3(a), GPT-4\\nshows the great capability of translating English\\nto Chinese and vice versa. From the radar chart,\\nwe can see that GPT-4 makes almost the same or\\nslightly fewer semantic errors (Omission, Addition,\\nand Mistranslation errors) than Junior and Senior\\ntranslators. Especially mistranslation errors, which\\nare generally considered most semantically detri-\\nmental, are better than junior and senior translators.\\nFor omission and addition errors, GPT-4 reaches al-\\nmost the same level as senior translators. However,\\nGPT-4 made significantly more lexical, stylistic,\\nand grammatical errors than human translators do.\\nThe error distribution of translation of GPT-4 meets\\nour expectations, as in the absence of reference,\\nGPT-4 will translate unfamiliar words directly and\\nliterally instead of seeking online materials or other\\nforms of help like human translators. Furthermore,\\ndue to the complexity and variability of Chinese,\\nthe translation of entity names or proper nouns is\\nusually not one-to-one, two above reasons together'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='Mistranslation(130)\\nAddition(4)\\nMT Hallucination(14)\\nOmission(29)\\nUntranslated(0)\\nWrong Name Entity & T erm(78)\\nGrammar(9)\\nPunctuation(11)\\nSpelling(4)\\nRegister(8)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(a) Chinese↔English\\nMistranslation(20)\\nAddition(8)\\nMT Hallucination(0)\\nOmission(17)\\nUntranslated(2)\\nWrong Name Entity & T erm(1)\\nGrammar(5)\\nPunctuation(3)\\nSpelling(3)\\nRegister(0)\\nInconsistent Style(3)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior (b) English↔Russian\\nMistranslation(52)\\nAddition(6)\\nMT Hallucination(5)\\nOmission(9)\\nUntranslated(6)\\nWrong Name Entity & T erm(7)\\nGrammar(6)\\nPunctuation(10)\\nSpelling(3)\\nRegister(1)\\nInconsistent Style(1)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\nmedium\\nsenior (c) Chinese↔Hindi\\nFigure 3: Error category results for each language. Each sub-figure is the average over two directions.\\nWe only include ‘Major’ errors here to highlight the most severe problems. Higher values indicate more\\nerrors and the number after each error type is the maximum number of that error.\\ncause the inferiority of the performance of GPT-4\\nin these aspects.\\nEnglish-Russian For the English-Russian trans-\\nlation tasks, GPT-4 made slightly more semantic\\nerrors but the number of mistranslation errors made\\nby GPT-4 is almost at the same level as medium\\nand senior translators. However, GPT-4 generally\\nmade less stylistic, grammatical, and wrong name\\nentity & term than junior translators. The English-\\nRussian translation tasks are quite challenging and\\nthe performance of translators varies significantly,\\nbut GPT-4 still maintains the average level overall.\\nHindi-Chinese As the low-resource language\\npair we evaluate, GPT-4 demonstrates the worst\\nperformance across evaluated translators. We ob-\\nserve that GPT4 is inferior to our MT baseline. This\\nmay be due to the small portion of Hindi and Chi-\\nnese corpora in its pre-training dataset. Specifically,\\nmaking the most ‘Mistranslation’ errors of GPT-4\\nindicates a distance away from the language under-\\nstanding of human translators. As a comparison,\\nSeamlessM4T performs better in both semantic and\\nlexical errors.\\nDiscussion Our results here manifest an imbal-\\nance of multilinguality for LLMs (Wang et al.,\\n2023b). Our results imply that GPT-4 can serve\\nas a reliable translator for resource-high such as\\nChinese to English but is doubtful for low-resource\\ndirections like Chinese-Hindi. In the low-resource\\nscenario, machine translator is more reliable.\\n5.3 Detailed Results for Different Domains\\nFigure 4 presents our results for different domains\\nin Chinese-to-English translation. We compare\\nthree different domains, including news, technol-\\nogy, and biomedical.\\nGeneral News Domain GPT-4 performs worse\\nin the news domain than human translators of three\\nlevels. The number of semantic errors made by\\nGPT-4 is quite close to junior and medium transla-\\ntors. Nonetheless, GPT-4 made more lexical and\\ngrammatical errors compared to human translators.\\nWe hypothesize the reasons for the situation de-\\nscribed above to happen are mainly because of the\\nliterariness and timeliness. Because GPT-4 is not\\nable to access the online materials to confirm the\\nname of a specific entity or event.\\nTechnology Domain The performance of GPT-4\\nis relatively close to medium-level translators. Ex-\\ncept for the Wrong Name Entity & Terms, GPT-4\\nmakes almost the same or even fewer errors than\\nmedium-level translators across all aspects. Specif-\\nically, the number of semantic errors made by GPT-\\n4 is almost the same to medium-level translators\\nand it makes much fewer structural and grammati-\\ncal errors. It means that in this field, GPT-4 might\\nunderstand the original text better than junior or\\nmedium-level translators and be able to conduct\\na translation that is more in line with the original\\nmeaning.\\nBiomedical Domain Similar to the technology\\ndomain, the qualities of the translations made by\\nGPT-4 and medium-level translators stand at the'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='Mistranslation(105)\\nAddition(4)\\nMT Hallucination(2)\\nOmission(27)\\nUntranslated(0)\\nWrong Name Entity & T erm(75)\\nGrammar(9)\\nPunctuation(7)\\nSpelling(2)\\nRegister(6)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(a) General news domain.\\nMistranslation(114)\\nAddition(3)\\nMT Hallucination(2)\\nOmission(57)\\nUntranslated(0)\\nWrong Name Entity & T erm(157)\\nGrammar(30)\\nPunctuation(9)\\nSpelling(3)\\nRegister(8)\\nInconsistent Style(5)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior (b) Technology domain.\\nMistranslation(21)\\nAddition(0)\\nMT Hallucination(3)\\nOmission(9)\\nUntranslated(0)\\nWrong Name Entity & T erm(49)\\nGrammar(0)\\nPunctuation(2)\\nSpelling(1)\\nRegister(0)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior (c) Biomedical domain.\\nFigure 4: Error category results for different domains in Chinese-to-English. We only include ‘Major’\\nerrors here to highlight the most severe problems. Higher values indicate more errors and the number in\\nthe bracket is the maximum number of that error.\\nSource 巨人网络有限公司\\nGPT-4 Giant Network Group Inc.\\nHuman Giant Interactive Group Inc.\\nTable 5: Named Entity cases.\\nsame level. Despite slightly more Wrong Name\\nEntity & Terms errors made, GPT-4 performs better\\nthan junior and medium-level translators in other\\naspects.\\nDiscussion For specific domains like technol-\\nogy, we show that GPT-4 is comparable with ju-\\nnior/medium translators. We still notice a similar\\nimbalance issue as in the multilingual setting, but\\nGPT-4’s performance is not as sensitive as in the\\nchange of language.\\n5.4 Case Study\\nWe also qualitatively understand the difference be-\\ntween the translations given by GPT-4 and human\\ntranslators.\\nLiteral Translations Among the error cases, the\\ntypical one is literal translations. Specifically, we\\nfind that GPT-4 sometimes translates with semanti-\\ncally correct, but in-native and literal translations.\\nThis is problematic with named entities, especially\\nthose occurring less frequently. As shown in Table\\n5, when not knowing the correct translation of ‘巨\\n人网络有限公司’, GPT-4 translates the term word\\nby word. However, the issue of name entities oc-\\ncurs less for human translators, partially because\\nthey would google it to find the correct translation.\\nSource It’s just a white screen or it times out load-\\ning it, or the page becomes unresponsive!\\nGPT-4\\n它只是一个白屏，要么是加载时超时，\\n要么页面变得无响应了！\\nHuman\\n页面要么显示空白，要么加载超时或是\\n无响应。\\nTable 6: Unnatural-Flow cases. Red represents the\\nliteral translation and green is more natural and\\nnative in Chinese.\\nThus, this issue might be resolved by incorporating\\nweb-search into agent-like translation (Feng et al.,\\n2024; Wu et al., 2024c).\\nExcept for named entities, we notice that the\\nliteral translation causes Unnatural Flows. As\\nshown in Table 6, when translating ‘It’s just a white\\nscreen’, GPT-4 translates the phrase to ‘ 它(it)只\\n是(is just)一个(a)白屏(white screen)’, but human\\ntranslator translates this phrase to ‘‘ 页面显示空\\n白(The page display is white)’’, which represents\\na preciser meaning and follows local conventions.\\nHuman Imagination We find human translators\\nalso have drawbacks compared to the GPT-4 trans-\\nlator. When the source sentence contains insuffi-\\ncient information to translate, human translators\\ntend to fill the gap by imagination or overthinking.\\nAn example is given in Table 7. The translator\\nwrongly understands the phrase ‘entering his 2nd\\nyear’ as Daley is a two-year-old baby, but the sen-\\ntence describes a 2nd-year player for sports. This\\nmay be due to daily language habits, misunder-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='Source He has health concerns atm but\\nwe also have Daley entering his 2nd year\\nand is a decent safety net.\\nGPT-4 他目前有健康问题，但我们还有戴利进\\n入他的第二年，他是一个不错的安全保\\n障。\\nHuman 他目前有健康问题。不过，戴利两岁\\n了，是个不错的备选人。\\nTable 7: Human imagination cases. Red denotes the\\nimagined part.\\nstanding, or not paying attention, and could be\\nrelated to the hallucination (Zhang et al., 2023) of\\nLLMs. GPT-4’s literal translation helps in this, as\\nit keeps faithful to the source sentence. This also\\naligns with our findings in Section 5.1 that GPT-4\\nhas fewer Additions or Omissions.\\n6 Conclusion\\nIn this study, we comprehensively evaluated the\\ntranslation quality of GPT-4 against human trans-\\nlators of varying expertise levels across multiple\\nlanguage pairs and domains. Our findings showed\\nthat GPT-4 performs comparably to junior transla-\\ntors in terms of total errors made but lags behind\\nmedium and senior translators. We also notice that\\nGPT-4’s translation capability gradually weakens\\nfrom resource-rich to resource-poor language pairs.\\nQualitative analysis revealed that GPT-4 tends to\\nproduce more literal translations compared to hu-\\nman translators but suffers less from imagined in-\\nformation.\\nThe results of this study demonstrate that GPT-4\\nhas made significant strides in approaching human-\\nlevel translation quality, as well as highlighting the\\nnuanced difference between them. This suggests\\npromising opportunities for collaboration and en-\\nhancement of translation workflows. As research\\ncontinues to advance, we anticipate that LLMs will\\nbecome increasingly valuable tools in the trans-\\nlation industry, working alongside human transla-\\ntors to improve productivity, efficiency, and overall\\ntranslation quality.\\n7 Limitations\\nOur work is limited in the following aspects: (1)\\nWe benchmark GPT-4 for translation tasks, as it is\\na representative large language model and shows\\nstate-of-the-art performance for many text-based\\ntasks. However, our evaluations can be extended\\nto other LLMs such as Claude-3. (2) Our eval-\\nuation covers three languages and six directions\\nfrom resource-rich to resource-poor. However, for\\nother languages, there might be linguistic-specific\\nphenomena that are not covered in this paper.\\nReferences\\nGuangsheng Bao, Yanbin Zhao, Zhiyang Teng,\\nLinyi Yang, and Yue Zhang. 2023. Fast-\\ndetectgpt: Efficient zero-shot detection of\\nmachine-generated text via conditional probabil-\\nity curvature. arXiv preprint arXiv:2310.05130.\\nLjubisa Bojic, Predrag Kovacevic, and Milan\\nCabarkapa. 2023. Gpt-4 surpassing human\\nperformance in linguistic pragmatics. arXiv\\npreprint arXiv:2312.09545.\\nJacob Cohen. 1960. A coefficient of agreement for\\nnominal scales. Educational and psychological\\nmeasurement, 20(1):37–46.\\nSeamless Communication, Loïc Barrault, Yu-An\\nChung, Mariano Cora Meglioli, David Dale,\\nNing Dong, Paul-Ambroise Duquenne, Hady\\nElsahar, Hongyu Gong, Kevin Heffernan, John\\nHoffman, Christopher Klaiber, Pengwei Li,\\nDaniel Licht, Jean Maillard, Alice Rakotoari-\\nson, Kaushik Ram Sadagopan, Guillaume Wen-\\nzek, Ethan Ye, Bapi Akula, Peng-Jen Chen,\\nNaji El Hachem, Brian Ellis, Gabriel Mejia\\nGonzalez, Justin Haaheim, Prangthip Hansanti,\\nRuss Howes, Bernie Huang, Min-Jae Hwang,\\nHirofumi Inaguma, Somya Jain, Elahe Kalbassi,\\nAmanda Kallet, Ilia Kulikov, Janice Lam, Daniel\\nLi, Xutai Ma, Ruslan Mavlyutov, Benjamin Pelo-\\nquin, Mohamed Ramadan, Abinesh Ramakrish-\\nnan, Anna Sun, Kevin Tran, Tuan Tran, Igor\\nTufanov, Vish V ogeti, Carleigh Wood, Yilin\\nYang, Bokai Yu, Pierre Andrews, Can Balioglu,\\nMarta R. Costa-jussà, Onur Celebi, Maha El-\\nbayad, Cynthia Gao, Francisco Guzmán, Justine\\nKao, Ann Lee, Alexandre Mourachko, Juan Pino,\\nSravya Popuri, Christophe Ropers, Safiyyah\\nSaleem, Holger Schwenk, Paden Tomasello,\\nChanghan Wang, Jeff Wang, and Skyler Wang.\\n2023. Seamlessm4t: Massively multilingual &\\nmultimodal machine translation.\\nMaxim Enis and Mark Hopkins. 2024. From\\nllm to nmt: Advancing low-resource ma-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='chine translation with claude. arXiv preprint\\narXiv:2404.13813.\\nAkhbardeh Farhad, Arkhangorodsky Arkady,\\nBiesialska Magdalena, Bojar Ondˇrej, Chatterjee\\nRajen, Chaudhary Vishrav, Marta R Costa-jussa,\\nEspaña-Bonet Cristina, Fan Angela, Federmann\\nChristian, et al. 2021. Findings of the 2021 con-\\nference on machine translation (wmt21). In Pro-\\nceedings of the Sixth Conference on Machine\\nTranslation, pages 1–88. Association for Com-\\nputational Linguistics.\\nZhaopeng Feng, Yan Zhang, Hao Li, Bei Wu, Jiayu\\nLiao, Wenqiang Liu, Jun Lang, Yang Feng, Jian\\nWu, and Zuozhu Liu. 2024. Tear: Improving\\nllm-based machine translation with systematic\\nself-refinement.\\nLukas Fischer and Samuel L¨\"aubli. 2020. What’s\\nthe difference between professional human and\\nmachine translation? a blind multi-language\\nstudy on domain-specific MT. In Proceedings\\nof the 22nd Annual Conference of the European\\nAssociation for Machine Translation, pages 215–\\n224, Lisboa, Portugal. European Association for\\nMachine Translation.\\nMarkus Freitag, George Foster, David Grang-\\nier, Viresh Ratnakar, Qijun Tan, and Wolfgang\\nMacherey. 2021. Experts, errors, and context:\\nA large-scale study of human evaluation for ma-\\nchine translation. Transactions of the Associ-\\nation for Computational Linguistics , 9:1460–\\n1474.\\nMarkus Freitag, Nitika Mathur, Chi-kiu Lo, Eleft-\\nherios Avramidis, Ricardo Rei, Brian Thompson,\\nTom Kocmi, Frederic Blain, Daniel Deutsch,\\nCraig Stewart, Chrysoula Zerva, Sheila Castilho,\\nAlon Lavie, and George Foster. 2023. Results\\nof WMT23 metrics shared task: Metrics might\\nbe guilty but references are not innocent. In Pro-\\nceedings of the Eighth Conference on Machine\\nTranslation, pages 578–628, Singapore. Associ-\\nation for Computational Linguistics.\\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-\\nkiu Lo, Craig Stewart, Eleftherios Avramidis,\\nTom Kocmi, George Foster, Alon Lavie, and\\nAndré F. T. Martins. 2022. Results of WMT22\\nmetrics shared task: Stop using BLEU – neu-\\nral metrics are better and more robust. In Pro-\\nceedings of the Seventh Conference on Machine\\nTranslation (WMT), pages 46–68, Abu Dhabi,\\nUnited Arab Emirates (Hybrid). Association for\\nComputational Linguistics.\\nTanya Goyal, Junyi Jessy Li, and Greg Durrett.\\n2022. News summarization and evaluation in the\\nera of gpt-3. arXiv preprint arXiv:2209.12356.\\nYvette Graham, Timothy Baldwin, Alistair Mof-\\nfat, and Justin Zobel. 2013. Continuous mea-\\nsurement scales in human evaluation of machine\\ntranslation. In Proceedings of the 7th Linguis-\\ntic Annotation Workshop and Interoperability\\nwith Discourse, pages 33–41, Sofia, Bulgaria.\\nAssociation for Computational Linguistics.\\nYvette Graham, Christian Federmann, Maria Es-\\nkevich, and Barry Haddow. 2020. Assessing\\nhuman-parity in machine translation on the seg-\\nment level. In Findings of the Association for\\nComputational Linguistics: EMNLP 2020, pages\\n4199–4207, Online. Association for Computa-\\ntional Linguistics.\\nTianyu Han, Lisa C Adams, Keno Bressem, Fe-\\nlix Busch, Luisa Huck, Sven Nebelung, and\\nDaniel Truhn. 2023. Comparative analysis of\\ngpt-4vision, gpt-4 and open source llms in clin-\\nical diagnostic accuracy: A benchmark against\\nhuman expertise. medRxiv, pages 2023–11.\\nHany Hassan, Anthony Aue, Chang Chen, Vishal\\nChowdhary, Jonathan Clark, Christian Fed-\\nermann, Xuedong Huang, Marcin Junczys-\\nDowmunt, William Lewis, Mu Li, et al. 2018.\\nAchieving human parity on automatic chinese\\nto english news translation. arXiv preprint\\narXiv:1803.05567.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy\\nZou, Mantas Mazeika, Dawn Song, and Jacob\\nSteinhardt. 2021. Measuring massive multitask\\nlanguage understanding.\\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\\nVikas Raunak, Mohamed Gabr, Hitokazu Mat-\\nsushita, Young Jin Kim, Mohamed Afify, and\\nHany Hassan Awadalla. 2023. How good are gpt\\nmodels at machine translation? a comprehensive\\nevaluation. arXiv preprint arXiv:2302.09210.\\nHui Huang, Shuangzhi Wu, Xinnian Liang, Bing\\nWang, Yanrui Shi, Peihao Wu, Muyun Yang, and\\nTiejun Zhao. 2023. Towards making the most of'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='llm for translation quality estimation. In CCF\\nInternational Conference on Natural Language\\nProcessing and Chinese Computing, pages 375–\\n386. Springer.\\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang,\\nXing Wang, Shuming Shi, and Zhaopeng Tu.\\n2023a. Is chatgpt a good translator? yes with\\ngpt-4 as the engine.\\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang,\\nXing Wang, Shuming Shi, and Zhaopeng Tu.\\n2023b. Is chatgpt a good translator? yes with\\ngpt-4 as the engine.\\nAhrii Kim, Yunju Bak, Jimin Sun, Sungwon Lyu,\\nand Changmin Lee. 2023. The suboptimal\\nwmt test sets and its impact on human parity.\\nPreprints.\\nFilip Klubi ˇcka, Antonio Toral, and Víctor M\\nSánchez-Cartagena. 2018. Quantitative fine-\\ngrained human evaluation of machine transla-\\ntion systems: a case study on english to croatian.\\nMachine Translation, 32(3):195–215.\\nTom Kocmi, Eleftherios Avramidis, Rachel Baw-\\nden, Ond ˇrej Bojar, Anton Dvorkovich, Chris-\\ntian Federmann, Mark Fishel, Markus Freitag,\\nThamme Gowda, Roman Grundkiewicz, et al.\\n2023. Findings of the 2023 conference on ma-\\nchine translation (wmt23): Llms are here but\\nnot quite there yet. In Proceedings of the Eighth\\nConference on Machine Translation, pages 1–42.\\nTom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton\\nDvorkovich, Christian Federmann, Mark Fishel,\\nThamme Gowda, Yvette Graham, Roman Grund-\\nkiewicz, Barry Haddow, et al. 2022. Findings\\nof the 2022 conference on machine translation\\n(wmt22). In Proceedings of the Seventh Con-\\nference on Machine Translation (WMT), pages\\n1–45.\\nKlaus Krippendorff. 1980. Validity in content anal-\\nysis. Computerstrategien für die Kommunika-\\ntionsanalyse, 69:45p.\\nYafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue\\nWang, Linyi Yang, Shuming Shi, and Yue Zhang.\\n2023. Deepfake text detection in the wild. arXiv\\npreprint arXiv:2305.13242.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\\nJiang, Hiroaki Hayashi, and Graham Neubig.\\n2023a. Pre-train, prompt, and predict: A sys-\\ntematic survey of prompting methods in natural\\nlanguage processing. ACM Computing Surveys,\\n55(9):1–35.\\nZhengliang Liu, Tianyang Zhong, Yiwei Li, Yu-\\ntong Zhang, Yi Pan, Zihao Zhao, Peixin Dong,\\nChao Cao, Yuxiao Liu, Peng Shu, et al. 2023b.\\nEvaluating large language models for radiology\\nnatural language processing. arXiv preprint\\narXiv:2307.13693.\\nArle Lommel, Maja Popovic, and Aljoscha Bur-\\nchardt. 2014. Assessing inter-annotator agree-\\nment for translation error annotation. In MTE:\\nWorkshop on Automatic and Manual Metrics for\\nOperational Translation Evaluation, pages 31–\\n37. Language Resources and Evaluation Confer-\\nence Reykjavik.\\nLaurence T Maloney, Maria F Dal Martello, Vi-\\nvian Fei, and Valerie Ma. 2024. A compar-\\nison of human and gpt-4 use of probabilistic\\nphrases in a coordination game. Scientific re-\\nports, 14(1):6835.\\nHiroki Nakayama, Takahiro Kubo, Junya Ka-\\nmura, Yasufumi Taniguchi, and Xu Liang.\\n2018. doccano: Text annotation tool\\nfor human. Software available from\\nhttps://github.com/doccano/doccano.\\nHa Nguyen and Vicki Allan. 2024. Using gpt-4\\nto provide tiered, formative code feedback. In\\nProceedings of the 55th ACM Technical Sympo-\\nsium on Computer Science Education V . 1, pages\\n958–964.\\nKeqin Peng, Liang Ding, Qihuang Zhong, Li Shen,\\nXuebo Liu, Min Zhang, Yuanxin Ouyang, and\\nDacheng Tao. 2023. Towards making the most\\nof chatgpt for machine translation. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2023, pages 5622–5633.\\nThierry Poibeau. 2022. On\" human parity\" and\" su-\\nper human performance\" in machine translation\\nevaluation. In Language Resource and Evalua-\\ntion Conference.\\nRicardo Rei, Craig Stewart, Ana C Farinha, and\\nAlon Lavie. 2020a. COMET: A neural frame-\\nwork for MT evaluation. In Proceedings of the'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='2020 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP), pages 2685–\\n2702, Online. Association for Computational\\nLinguistics.\\nRicardo Rei, Craig Stewart, Ana C Farinha, and\\nAlon Lavie. 2020b. Comet: A neural frame-\\nwork for mt evaluation. In Proceedings of the\\n2020 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP), pages 2685–\\n2702.\\nSai Cheong Siu. 2023. Chatgpt and gpt-4 for pro-\\nfessional translators: Exploring the potential of\\nlarge language models in translation. Available\\nat SSRN 4448091.\\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy\\nWay. 2018. Attaining the unattainable? reassess-\\ning claims of human parity in neural machine\\ntranslation. In Proceedings of the Third Confer-\\nence on Machine Translation: Research Papers,\\npages 113–123, Brussels, Belgium. Association\\nfor Computational Linguistics.\\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui\\nZhang, Dian Yu, Shuming Shi, and Zhaopeng\\nTu. 2023a. Document-level machine translation\\nwith large language models.\\nWenxuan Wang, Wenxiang Jiao, Jingyuan Huang,\\nRuyi Dai, Jen-tse Huang, Zhaopeng Tu, and\\nMichael R Lyu. 2023b. Not all countries cel-\\nebrate thanksgiving: On the cultural dominance\\nin large language models. CoRR.\\nMinghao Wu, Thuy-Trang Vu, Lizhen Qu, George\\nFoster, and Gholamreza Haffari. 2024a. Adapt-\\ning large language models for document-\\nlevel machine translation. arXiv preprint\\narXiv:2401.06468.\\nMinghao Wu, Yulin Yuan, Gholamreza Haffari,\\nand Longyue Wang. 2024b. (perhaps) beyond\\nhuman translation: Harnessing multi-agent col-\\nlaboration for translating ultra-long literary texts.\\narXiv preprint arXiv:2405.11804.\\nMinghao Wu, Yulin Yuan, Gholamreza Haffari,\\nand Longyue Wang. 2024c. (perhaps) beyond\\nhuman translation: Harnessing multi-agent col-\\nlaboration for translating ultra-long literary texts.\\nHaoran Xu, Young Jin Kim, Amr Sharaf, and\\nHany Hassan Awadalla. 2023. A paradigm shift\\nin machine translation: Boosting translation per-\\nformance of large language models.\\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chen-\\njie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian\\nYu, Cong Yu, et al. 2020. Clue: A chinese\\nlanguage understanding evaluation benchmark.\\narXiv preprint arXiv:2004.05986.\\nFanghua Ye, Mingming Yang, Jianhui Pang,\\nLongyue Wang, Derek F. Wong, Emine Yilmaz,\\nShuming Shi, and Zhaopeng Tu. 2024. Bench-\\nmarking llms via uncertainty quantification.\\nLifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng\\nGao, Fangyuan Zou, Xingyi Cheng, Heng Ji,\\nZhiyuan Liu, and Maosong Sun. 2023. Revisit-\\ning out-of-distribution robustness in nlp: Bench-\\nmark, analysis, and llms evaluations.\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao\\nLiu, Tingchen Fu, Xinting Huang, Enbo Zhao,\\nYu Zhang, Yulong Chen, et al. 2023. Siren’s\\nsong in the ai ocean: a survey on hallucina-\\ntion in large language models. arXiv preprint\\narXiv:2309.01219.\\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein,\\nand Sameer Singh. 2021. Calibrate before use:\\nImproving few-shot performance of language\\nmodels. In International conference on machine\\nlearning, pages 12697–12706. PMLR.\\nJie Zhu, Junhui Li, Yalong Wen, and Lifan Guo.\\n2024. Benchmarking large language models\\non cflue–a chinese financial language under-\\nstanding evaluation dataset. arXiv preprint\\narXiv:2405.10542.\\nA Expertise of Human Annotators\\nTo categorize translators into junior, medium, or\\nsenior levels, we have established a comprehensive\\nset of criteria that take into account various factors\\nindicative of a translator’s expertise and experience.\\nThese factors include the translator’s educational\\nbackground, particularly the prestige of the insti-\\ntution from which they graduated, as well as their\\nlength of service in the translation industry, the\\nduration of their translation career, the number of\\ntranslations completed, and any professional certifi-\\ncations they have obtained. To ensure the ongoing\\ncompetence of our translators, we conduct quar-\\nterly assessments to evaluate their performance.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='For instance, to be classified as a senior-level trans-\\nlator, an individual must possess a minimum of\\nten years of translation experience, demonstrate\\nexceptional proficiency by achieving a score of\\n99% on our assessments, and hold the distinguished\\nCATTI++ translation certification. By considering\\nthese stringent criteria, we aim to maintain a highly\\nqualified and skilled pool of translators across all\\nlevels of expertise.\\nB Annotation Requirements\\nB.1 Error Types\\nOur annotation system is built upon the open-\\nsourced doccano system 3. In Figure 5, we provide\\na screenshot of our annotation system. For each\\nsource sentence, outputs for different systems are\\ngiven and the annotators can select spans of the text\\nand annotate the error type and severity.\\nC Detailed Explanation and Guidance for\\nEach Error Types\\nOur evaluation protocol largely follows the MQM\\ncriteria released by Unbabel 4. We provide a de-\\ntailed annotation manual for annotators, including\\nan explanation for each error type as well as illus-\\ntrative examples for error types. It is included in\\nthe following:\\nC.1 Annotation Requirements\\nThe minimum unit that can be selected and anno-\\ntated is a whole word, a whitespace, a punctuation\\nmark, or an isolated character. In the following\\nexample, the version in French has an extra excla-\\nmation mark, so it’s necessary to annotate it as a\\nPunctuation error:\\n[EN] Thank you very much.\\n[FR] Merci beaucoup!\\nWrong selection →Merci [beau-\\ncoup!]PUNCTUATION\\nCorrect selection →Merci beau-\\ncoup[!]PUNCTUATION\\nIf the issue occurs in a multiword expression,\\nyou will need to select the whole expression; if, for\\nexample, an entire sentence was translated and it\\nshouldn’t have been, you should select the entire\\nsentence.\\n3https://github.com/doccano/doccano\\n4\\nIn the following example, we have an Unnatural\\nFlow error:\\n[EN] Hi, Mary here.\\n[ES] Hola, Mary aquí.\\nWrong selection → Hola, [Mary\\naquí.]UNNATURAL FLOW\\nCorrect selection → Hola, [Mary\\naquí]UNNATURAL FLOW.\\nC.2 Error Types\\nAccuracy\\n• Mistranslation\\n– Description: Translation does not accu-\\nrately represent the source.\\n– Example:\\n[EN] It has to be done by the book.\\n[FR] Il doit être fait [par le\\nlivre]MISTRANSLATION\\n[Reason] The word-for-word trans-\\nlation into French doesn’t work.\\n• Addition\\n– Description: Information not present in\\nthe source.\\n– Example:\\n[EN] That way you can be sure that\\nyou were the one who made the\\nchanges.\\n[ES] Así puedes estar seguro de que\\nfuiste tú quien hizo [todos ADDI-\\ntIoN los cambios.\\n[Reason] [Todos] (meaning ’all’ in\\nSpanish) is not present in the source\\nand it is incorrectly added in the\\ntarget text.\\n• MT Hallucination\\n– Description: information that has noth-\\ning related to source; or gibberish; or\\nrepeats\\n– Example:\\n[EN] You can send us a follow-up\\nemail at this address [EMAIL].\\n[ES] [Hágame saber si tiene al-\\nguna otra pregunta]MT HALLUCI-\\nNATION.]\\n[Reason]: The Spanish translation'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='Figure 5: A screenshot of the Doccano annotation system we use.\\nreads please let me know if you\\nhave any other questions and it’s\\ngrammatically correct and fluent,\\nbut it has no relation at all with the\\nsource.]\\n• Omission\\n– Description: Missing content from the\\nsource.\\n– Example:\\n[EN] We do not have much informa-\\ntion on this.\\n[FR] Nous ne disposons\\npas [] OMISSION beaucoup\\nd’informations à ce sujet.\\n[Reason]: The French sentence\\nrequires the preposition [de] (dis-\\nposer de).\\n• Untranslated\\n– Description: Not translated.\\n– Example:\\n[EN] How To Make Pizza Dough\\n[FR] Comment faire de [Pizza\\nDough|UNTRANSLATED\\n[Reason]: [Pizza Dough] is not a\\nnamed entity and is untranslated in\\nthe French version.\\n• Wrong Name Entity & Term\\n– Description: Wrong usage of NE and\\nTerminology.\\n– Example:\\n[EN] Dear Wiley,\\n[IT] Gentile [Wilar WRONG\\nNAMED ENTITY,\\n[Reason]: The name in the Italian\\nversion doesn’t match the original.\\nFluency\\n• Grammar\\n– Description: Problems with grammar of\\ntarget language.\\n– Example:\\n[EN] I understand that you want to\\ncheck in online.\\n[CS] chàpu, ze se chcete\\n[odbavení]gRAMMaR online.\\n[Reason]: Wrong part of speech\\nmakes the sentence ungrammatical\\nin Czech.\\n• Punctuation\\n– Description: incorrect punctuation (for\\nlocale or style).\\n– Example:'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='[EN] Original copy of the Proof of\\nPurchase or Invoice (not a screen-\\nshot):\\n[PT] C’opia original do com-\\nprovante de compra ou nota\\nfiscal (não uma captura de\\ntela)[.]PUNCTUATION\\n[Reason]: There’s a period instead\\nof a colon in the Brazilian Por-\\ntuguese version of this sentence.\\n• Spelling\\n– Description: incorrect spelling or capital-\\nization.\\n– Example:\\n[EN] This sort of damage is not cov-\\nered under the warranty, but we will\\nseek assistance from a higher sup-\\nport and see what we can do regard-\\ning this issue.\\n[IT] Questo tipo di danno non è cop-\\nerto dalla garanzia, ma chiederò\\ncomunque aiuto ai responsabili\\ndell’assistenza per capire che cosa\\n[Zi]SPELLING può fare per quanto\\nriguarda questo problema.\\n[Reason]: There’s a typo in the\\nsentence in Italian: the word [zi]\\nshould be [si] instead.\\n• Register\\n– Description: Wrong grammatical regis-\\nter (e.g., inappropriately informal pro-\\nnouns).\\n– Example:\\n[EN] Wishing you a great day\\nahead.\\n[DE] Ich wünsche [Ih-\\nnen]REGISTER einen schönen\\nTag.\\n[Reason]: The required register for\\nthe German translation is Informal\\nbut the pronoun [Inhen] is Formal.\\n• Inconsistent Style\\n– Description: internal inconsistency (not\\nrelated to terminology).\\n– Example:\\n[EN] Please click on this link. [...]\\nThis link will expire in 24 hours.\\n[NN] Klikk på denne\\n[lenken].[...]Denne\\n[linken]INCONSISTENCY ut-\\nloper om 24 timer.\\n[Reason]: Both [lenk] and [link]\\nare correct in Norwegian, but in the\\nsame document, only one should be\\nused. Note: this is a single error,\\nnot two\\n• Unnatural Flow\\n– Description: translations that are too lit-\\neral or sound unnatural.\\n– Example:\\n[EN] Zebras are ideal for animal\\nmatching.\\n[DE] [Zebras sind ideal,\\num bestimmte Tiere zu\\nfinden]UNNATURAL FLOW.\\n[Reason] The German translation\\nsounds too literal, it reads like a\\ntranslation, using the verb [finden]\\n(finding) as a translation for match-\\ning. The verb matching should be\\ntranslated as [detektieren] (detect)\\nto read as if it was originally written\\nin the target language: [Zebras sind\\nein ideales Beispiel zur Detektion\\nvon Wildtieren.]\\nOther\\n• Non-translation\\nD Extra Details\\nD.1 Translation Prompt in Preliminary Study\\nIn two experiments, the translation prompt we use\\nis as follows:\\n• Please translate the following sentences from\\n<SRC_LANG> to <TGT_LANG>. Ensure\\nline alignment across the document while\\nmaintaining the fluency of overall translation.\\nThe prompt asks GPT4 to maintain the sentence\\nalignment of the given document, so each sentence\\ncan be aligned back to its source sentence while be-\\ning translated at the document level. In practice, we\\nfind most times GPT4 can follow our instructions.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='Occasionally, it fails to keep the sentence structure\\nof the document and merges some sentences in one\\nrow. In these cases, we manually split the merged\\nsentences.\\nD.2 Model and Decoding\\nFor GPT-4, we use greedy search for decoding,\\nto ensure the reproducibility of the results. For\\nSeamlessM4T, we use the 2.3B version of seam-\\nlessM4T_v2_large and adopt beam search with\\nbeam size 5.')]\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='GPT-4 vs. Human Translators: A Comprehensive Evaluation of\\nTranslation Quality Across Languages, Domains, and Expertise Levels\\nJianhao Yan1,2∗ Pingchuan Yan3∗ Yulong Chen4∗\\nJudy Li5 Xianchao Zhu5 Yue Zhang2,6,\\x00\\n1 Zhejiang University 2 School of Engineering, Westlake University\\n3 University College London 4 University of Cambridge 5 Lan-Bridge Group\\n6 Institute of Advanced Technology, Westlake Institute for Advanced Study\\nelliottyan37@gmail.com\\nAbstract\\nThis study comprehensively evaluates the\\ntranslation quality of Large Language Mod-\\nels (LLMs), specifically GPT-4, against hu-\\nman translators of varying expertise lev-\\nels across multiple language pairs and do-\\nmains. Through carefully designed annota-\\ntion rounds, we find that GPT-4 performs\\ncomparably to junior translators in terms of\\ntotal errors made but lags behind medium\\nand senior translators. We also observe the\\nimbalanced performance across different lan-\\nguages and domains, with GPT-4’s transla-\\ntion capability gradually weakening from\\nresource-rich to resource-poor directions. In\\naddition, we qualitatively study the trans-\\nlation given by GPT-4 and human transla-\\ntors, and find that GPT-4 translator suffers\\nfrom literal translations, but human trans-\\nlators sometimes overthink the background\\ninformation. To our knowledge, this study\\nis the first to evaluate LLMs against human\\ntranslators and analyze the systematic differ-\\nences between their outputs, providing valu-\\nable insights into the current state of LLM-\\nbased translation and its potential limitations.\\n1 Introduction\\nRecent studies show that LLMs can serve as a\\nstrong translation system and a good substitute for\\nNMT models (Jiao et al., 2023a; Wang et al., 2023a;\\nEnis and Hopkins, 2024; Huang et al., 2023; Wu\\net al., 2024a; Hendy et al., 2023; Peng et al., 2023).\\nFor example, Jiao et al. (2023a) and Wang et al.\\n(2023a) find that GPT-4 can outperform commer-\\ncial machine translation systems via automatic and\\nhuman evaluations. Such impressive results have\\nhastened a wide range of applications, including\\nthe use of GPT-4 for literary translation (Wu et al.,\\n2024b).\\n∗These authors contributed equally to this work.\\nDespite their impressive capabilities, the nature\\nof LLM output compared to human translators\\nremains unclear. This raises two critical ques-\\ntions: (1) How do LLMs compare to human ex-\\nperts in translation quality? and (2) Are there\\nfundamental differences in their outputs? These\\ninquiries are particularly relevant in light of recent\\nresearch demonstrating significant distinctions be-\\ntween LLM-generated and human-generated texts\\nin general (Li et al., 2023; Bao et al., 2023). Such\\nfindings suggest that even if LLMs produce high-\\nquality translations, their outputs may possess\\nunique characteristics that distinguish them from\\nhuman-produced translations.\\nTo determine where LLMs fall within the spec-\\ntrum of human translation proficiency, which\\nranges from novice translators to seasoned profes-\\nsionals, we study the problem by taking the current\\nrepresentative LLM, i.e., GPT-4, and comparing\\nit against human translators with different exper-\\ntise. We first conduct a preliminary study compar-\\ning human translations against GPT-4 translations,\\nfinding that even experts cannot reach a consen-\\nsus on which translation is better . Given these\\nfindings, we take a finer-grained evaluation across\\ndifferent languages and domains, so that translation\\nquality can be better calibrated and systematic dif-\\nferences can be measured. Our evaluation covers\\nthree language pairs from resource-rich to resource-\\npoor, i.e., Chinese ↔English, Russian ↔English,\\nand Chinese↔Hindi, and three domains, i.e., News,\\nTechnology, and Biomedical. Given a source sen-\\ntence, we ask junior, medium, and senior trans-\\nlators and GPT-4 to generate the corresponding\\ntranslation in the target language. Then given each\\ntranslation pair, we hire independent expert annota-\\ntors to label the errors in the target sentence under\\nthe MQM schema (Freitag et al., 2021). We find\\nthat GPT-4 reaches a comparable performance to\\njunior translators in the perspective of total errors\\nmade, and lags behind senior ones with a consider-\\narXiv:2407.03658v1  [cs.CL]  4 Jul 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='able gap.\\nOur further analyses and qualitative studies show\\nthat there are imbalanced performances for differ-\\nent languages and domains. From resource-rich to\\nresource-poor directions, GPT-4’s translation capa-\\nbility gradually weakens. For resource-rich direc-\\ntions like Chinese↔English, GPT-4 performs com-\\nparably with junior translators and even close to\\nmedium translators, but in Chinese↔Hindi, it even\\nlags behind our baseline system. The weaknesses\\nmentioned above are also general shortcomings of\\nlarge models and reflect that although large models\\nhave achieved universal translation with a focus\\non one language, translation between low-resource\\nlanguages remains a relative weakness.\\nTo our knowledge, we are the first to evaluate\\nLLMs against human translators and analyze the\\nsystematic differences between LLMs and human\\ntranslators.\\n2 Related Work\\nBenchmarking LLMs Previous studies have\\nbenchmarked LLMs on various NLP tasks. Xu et al.\\n(2020) benchmark several LLMs on Chinese text,\\nevaluating their Chinese ability. Ye et al. (2024)\\nassess LLMs through Question Answering (QA),\\nMMLU (Hendrycks et al., 2021), and other metrics.\\nFrom these tests, LLMs with larger scales are gen-\\nerally proved to be more accurate except for certain\\ntasks. Yuan et al. (2023) demonstrates that LLMs\\nperform well in long-context understanding and\\nare more capable with Out-of-Distribution, which\\nmeans LLMs have a certain degree of generaliza-\\ntion ability.\\nFurther to the MT field, Jiao et al. (2023b) find\\nthat GPT-4 performed competitively with other\\nSotA translation products. Wang et al. (2023a)\\nfurther investigated the capability of GPT-4 in\\ndocument-level translation, the results show that\\nGPT-4 performs better than commercial translation\\nproducts and document NMT methods. Compared\\nto them, our work empirically shows that GPT-4 is\\ncomparable to junior human translators.\\nLLMs as Human ExpertsDue to the great ca-\\npacities of GPT-4 over traditional NLP models,\\nresearchers have investigated and compared the\\nperformance of GPT-4 as human experts in mul-\\ntiple NLP tasks. Zhu et al. (2024) highlight that\\nGPT-4 and GPT-4-turbo show top performance on\\na Chinese financial language understanding task.\\nLiu et al. (2023b) find the LLMs can be benefi-\\ncial to biomedical NLP tasks. Goyal et al. (2022)\\ncompare GPT models with several summarization\\nmodels and humans, and find that GPT can gen-\\nerate summaries preferred by humans. In AI for\\neducation area, Nguyen and Allan (2024) show\\nGPT-4’s can provide teaching feedback for stu-\\ndents. Maloney et al. (2024) find that GPT-4 shows\\nclose performance compared with human partici-\\npants in coordination games. Siu (2023) show that\\nGPT-4 is comparable to humans on technical trans-\\nlation tasks. Bojic et al. (2023) find that GPT-4 can\\noutperform human experts on linguistic pragmatic\\ntasks. In clinical diagnostics, Han et al. (2023)\\nfind that GPT-4 can give comparable performance\\nto humans, and GPT-4v (vision version) can even\\noutperform human experts.\\nHuman Evaluation for MT (Graham et al.,\\n2013) first propose Direct Assessment (DA), which\\nuses a continuous score from 0 to 100 to repre-\\nsent the quality of a hypothesis. DA has been\\nadopted in WMT translation tasks for the past\\nfew years (Farhad et al., 2021; Kocmi et al., 2022,\\n2023). MQM (Lommel et al., 2014), the annota-\\ntion used in this paper, is another widely used an-\\nnotation scheme (Klubiˇcka et al., 2018; Rei et al.,\\n2020a). It requires the annotators to annotate the\\nerror span for each hypothesis and is shown to be\\nmore accurate and reliable than DA (Freitag et al.,\\n2021). Thus, it is utilized in the metrics tasks of\\n2022 and 2023 WMT challenges (Freitag et al.,\\n2022, 2023).\\nHuman Parity The human parity for machine\\ntranslation systems is first claimed by (Hassan et al.,\\n2018), which describes a comparable performance\\non the WMT 2017 news translation task from Chi-\\nnese to English when compared to professional hu-\\nman translations. However, this claim is challenged\\nby the following research, raising concerns about\\nthe limited scope of human parity. These limita-\\ntions include the expertise of human evaluators (Fis-\\ncher and L¨\"aubli, 2020), the origin and quality of\\nsource sentences (Toral et al., 2018; Kim et al.,\\n2023), the limited scenario of comparison (Poibeau,\\n2022) and difficulty of translation (Graham et al.,\\n2020), indicating significant gaps between NMT\\nmodels and the professional translators. In this\\nwork, we evaluate whether the SOTA LLM GPT-\\n4 performs comparable to professional translators\\nand what differs between human translators and\\nLLMs. With the above lessons in mind, we address'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='these limitations by hiring expert annotators, avoid-\\ning target-origin source text, manually evaluating\\nsource sentences, and covering high-resource to\\nlow-resource language pairs and various domains.\\n3 Preliminary Study\\nThis section presents our preliminary study. We\\naim to first compare GPT-4 translations with hu-\\nman translations qualitatively, in a coarse manner.\\nOur comparison is simple and direct. We sample\\nhuman-translated texts and prompt GPT-4 to trans-\\nlate the same source sentence. Then, we ask expert\\nannotators to determine which translation is better.\\nParticularly, to have a quick overview of the qual-\\nities of human translations against GPT-4 transla-\\ntions, we first utilize COMET-QE 1 to score our\\nin-house Chinese to English human-translated doc-\\numents, and select two documents with the highest\\nscore and the lowest score. Note that our in-house\\ntranslated documents are all translated by profes-\\nsional translators. In this way, we gather 40 pairs\\nof translations from professional translators and\\nGPT-4, respectively. Recent findings (Freitag et al.,\\n2021) have demonstrated that crowd-sourced hu-\\nman ratings are less reliable for high-quality MT\\nevaluation. Thus, we hire six expert annotators to\\ncompare the two translations and select the better\\ntranslations they find. We randomly shuffle the\\nGPT-4 and human translations to prevent annota-\\ntors from identifying GPT-4.\\nThe average win rate of GPT is\\n15.5/40 (36.25%). It looks like a clear win\\nfor human translators, but when delving deeper,\\nwe find that the expert annotators have a low\\nratio of agreement with each other. In Table 1,\\nmost annotators only agree with each other at\\naround 60% (the baseline is 50%) of an agreed\\nwinner at each source sentence. We further\\nconduct a significance test and only annotator\\nB finds human translation significantly better\\nthan GPT’s translation and other annotators\\nhave high p-values. Given annotators’ expertise\\nand our task is straightforward, these results\\nindicate that even expert annotators find it difficult\\nto agree on which translation is better , and\\nGPT-generated translations might have different\\nadvantages against human-generated ones. These\\nresults motivate us to conduct a finer-grained and\\ncomprehensive evaluation to reveal the systematic\\ndifference between GPT-4 and human translations.\\n1Unbabel/wmt23-cometkiwi-da-xl\\nAnnotators A B C D E F\\nA 100.0 57.5 65.0 65.0 62.5 67.5\\nB - 100.0 52.5 52.5 50.0 50.0\\nC - - 100.0 65.0 82.5 67.5\\nD - - - 100.0 57.5 62.5\\nE - - - - 100.0 70.0\\nF - - - - - 100.0\\np-value 1.000 0.038 0.268 0.081 0.154 0.875\\nTable 1: Ratio(%) of agreed winner across expert\\nannotators and significance p-value for binomial\\ntest. P-value < 0.05 denotes a significant difference\\nbetween GPT-4 and Human.\\n4 Main Experimental Setup\\nMotivated by the results from our preliminary\\nstudy, we conduct a comprehensive and fine-\\ngrained evaluation, for revealing the systematic\\ndifference between humans and GPTs. Specifically,\\nwe employed the widely recognized Multidimen-\\nsional Quality Metrics (MQM) framework (Lom-\\nmel et al., 2014) and compared human translators\\nwith varying levels of expertise to GPT-4. Our\\nevaluation spans multiple languages and domains,\\naiming to furnish broad insights into these compar-\\nisons.\\n4.1 Data Collection\\nWe collect multilingual and multi-domain source\\nsentences. Our multilingual evaluation data con-\\ntains six language directions, covering high re-\\nsource to low resource, including English to Chi-\\nnese, Chinese to English, English to Russian, Rus-\\nsian to English, English to Hindi, and Hindi to\\nEnglish.\\nFor general domain Chinese ⇔English and\\nEnglish⇔Russian, we sample source sentences\\nfrom test sets of WMT2023 and WMT2022, re-\\nspectively. For Chinese⇔Hindi, we extract source\\nnews text from public websites. For multi-domain\\nevaluation data, we evaluate two domains, i.e.,\\nbiomedical and technology and we evaluate Chi-\\nnese to English. The source sentences are extracted\\nnews texts from public websites. We ensure that\\nall sources are source language origin to avoid the\\neffect of translationese. We manually evaluate all\\nsource sentences for these tasks and ensure the\\nsource sentences are not too easy or too short. Fi-\\nnally, each task contains 200 sentences, making our\\nevaluation a total of 1600 sentences.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='Type Error Name Explanations\\nAccuracy\\nMistranslation Translation does not accurately represent the source.\\nAddition Information not present in the source.\\nMT Hallucination Information that has nothing related to source; or gibber-\\nish; or repeats\\nOmission Missing content from the source.\\nUntranslated Not translated.\\nWrong Name Entity and Term Wrong usage of NE and Terminology.\\nFluency\\nGrammar Problems with grammar of target language.\\nPunctuation Incorrect punctuation (for locale or style)\\nSpelling Incorrect spelling or capitalization.\\nRegister Wrong grammatical register (e.g., inappropriately infor-\\nmal pronouns).\\nInconsistent Style Internal inconsistency ( not related to terminology )\\nUnnatural Flow Translations that are too literal or sound unnatural.\\nOther Non-translation -\\nTable 2: Error category and explanations. We mainly follow the guidelines from Unbabel, and merge some\\nerrors to reduce the efforts for annotators to understand the annotation system. Concrete examples for\\neach error category can be found in the Appendix.\\n4.2 Human Translators and Machine\\nTranslators\\nWe ask different human translators to translate our\\nsource sentences into the target language. Transla-\\ntors are of three different levels of expertise, cate-\\ngorized into junior-level, medium-level, and senior-\\nlevel translators. The level of expertise is ranked\\nby in-house criteria covering the translators’ edu-\\ncational background, translation experience, and\\npractical proficiency. See Appendix A for more de-\\ntails. For a fair comparison, we request the experts\\nnot use machine translation or GPTs as assistance.\\nFor all directions except Zh-Hi and Hi-Zh, we col-\\nlect three human translation results from each level\\nof expertise. For Zh-Hi and Hi-Zh, we only have\\nmedium-level and senior-level translators due to\\nthe scarcity of translators.\\nExcept for human translators, we use\\ngpt-4-1106-preview, the current state-\\nof-the-art large language model released by\\nOpenAI and Seamless M4T (Communication\\net al., 2023) as the representative of traditional ma-\\nchine translations to complement our experiments.\\nWe directly prompt GPT-4 to obtain the translation,\\nas it is the most common practice for normal users,\\nthe easiest to reproduce, and to avoid confusion by\\nvarious techniques.\\n4.3 Prompt Search\\nPrevious study (Zhao et al., 2021; Liu et al., 2023a)\\nshows that different prompts with LLMs can result\\nin distinctive performance. Thus, we collect three\\ncandidate prompts used in previous research (Xu\\net al., 2023; Jiao et al., 2023a) and use COMET-\\nQE (Rei et al., 2020b) to select the best prompt\\nto make the best use of GPT-4, as shown in Table\\n3. In particular, we use these three prompts to\\nprompt GPT-4 to translate 100 source sentences in\\nour Chinese-to-English test set and adopt COMET-\\nQE to evaluate the quality of translations. We find\\nthat the third prompt yields the best performance,\\nand hence we adopt this prompt for all following\\nexperiments.\\n4.4 Annotation Protocol\\nTo evaluate the results of candidates’ systems,\\nwe hire experts to annotate the errors of trans-\\nlations blindly. The annotation platform is Doc-\\ncano (Nakayama et al., 2018), and the error tags\\nare made according to MQM standards. MQM\\nrequires the annotators to annotate the span of er-\\nrors in each hypothesis. All hypotheses of the same\\nsource sentence are shown to the annotator together\\nto help decide which is better. We have 13 error\\ncategories and two severities, as shown in Table\\n2. Our categorization for errors mostly follows'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='Prompt COMET\\nPlease translate the following sen-\\ntence from Chinese into English.\\nYour language and style should align\\nwith the language conventions of a\\nnative speaker. \\\\n{SOURCE}\\\\n\\n0.775\\nYou are an expert translator for trans-\\nlating Chinese to English. Your\\nlanguage and style should align\\nwith the language conventions of\\na native speaker. \\\\n[Chinese]:\\n{SOURCE}\\\\n[English]:\\n0.755\\nPlease provide the English transla-\\ntion for these sentences. Your lan-\\nguage and style should align with\\nthe language conventions of a native\\nspeaker. \\\\n{SOURCE}\\\\n\\n0.780\\nTable 3: Taking Chinese to English as an example,\\nour three prompts and corresponding scores with\\nCOMET-QE.{SOURCE} represents the source sen-\\ntence to be translated.\\nUnbabel’s practice 2 and we focus on most com-\\nmon error types. Each tag has subtags with two\\nseverities, i.e., Minor or Major. A screenshot of the\\nannotation system is given in Figure 5.\\nFor each task, we first ask the two expert anno-\\ntators to carefully read our manual and conduct\\na training round on the first 10 groups of transla-\\ntions. Then, we manually check these annotations\\nto provide feedback and ask the two annotators to\\ncheck their disagreements and revise their results.\\nAfter two rounds of such training processes, we\\nask the annotators to finish the remaining sentences\\nwithout knowing each other’s results.\\nAfter the first round of annotation, we conduct a\\nsecond round to further refine the evaluation results.\\nIn particular, we hire another two experts for each\\ntask and show them the previous annotation results.\\nThey are asked to approve and make necessary\\nmodifications to previous round annotations.\\n4.5 Inter-Annotator Agreement\\nError annotation with MQM is challenging, and\\nprevious work demonstrates that the agreement\\nscores between MQM annotations are relatively\\nlow (Lommel et al., 2014). Reasons for this could\\nbe disagreement on precise spans and ambiguous\\n2https://help.unbabel.\\ncom/hc/en-us/articles/\\n6444304419479-Annotation-Guidelines-Typology-3-0\\nTask Cohen Kappa(Segment)Krippendorffs(Span)\\nReference, Re-Annotated by (Freitag et al., 2021)\\nWMT 2020 En-De 0.208 0.456\\nWMT 2021 En-De 0.230 0.501\\nOurs\\nGeneral Zh-En 0.257 0.436\\nGeneral En-Zh 0.544 0.579\\nGeneral En-Ru 0.461 0.566\\nGeneral Ru-En 0.341 0.875\\nGeneral Zh-Hi 0.256 0.443\\nGeneral Hi-Zh 0.234 0.495\\nTechnology Zh-En 0.306 0.581\\nBiomedical Zh-En 0.373 0.616\\nAverage 0.321 0.555\\nTable 4: Cohen Kappa (segment-level) and Krip-\\npendorffs’ Alpha (span-level) agreement of our an-\\nnotations.\\nerror categorization (Lommel et al., 2014). Despite\\nthe low agreement scores, MQM is more reliable\\nthan other evaluation protocols like Direct Assess-\\nment (Freitag et al., 2021).\\nTo compute inter-annotator agreement for MQM,\\nwe employ segment-level Cohen’s Kappa (Cohen,\\n1960) and span-level Krippendorff’s alpha (Krip-\\npendorff, 1980). For reference, we calculate the\\nagreement on the annotated results of the 2020 and\\n2021 WMT English-to-German tasks by (Freitag\\net al., 2021). Our IAA results are shown in Table 4.\\nThanks to our two-round annotation process, our\\nIAA scores show a favorable agreement, indicating\\na good annotation quality.\\n5 Main Results\\n5.1 Overall Results\\nAnalysis of Error Severity The upper part of\\nFigure 1 plots the averaged number of errors of\\ndifferent systems and translators. Compared to our\\nMT baseline (seamless), GPT-4 has much fewer er-\\nrors. It performs almost as well as the junior-level\\ntranslator at the level of total errors, as GPT-4 is\\nannotated with only slightly more minor and major\\nerrors than junior translators. However, GPT-4 still\\nhas clear performance gaps between medium or\\nsenior human translators, as it makes considerably\\nmore mistakes than experienced translators. To our\\nknowledge, we are the first to report how GPT-4 is\\non translation against human translators.\\nAnalysis of Error CategoriesFurthermore, we\\nplot the categories of errors in the bottom part of\\nFigure 1. Compared with junior human translators,\\nGPT-4 makes more errors in the accuracy of trans-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='seamless gpt4 junior medium senior\\nSystems\\n0\\n50\\n100\\n150\\n200Number of Errors\\nAverage Severity for Each System\\nMinor\\nMajor\\nseamless gpt4 junior medium senior\\nSystems\\n0\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200Number of Errors\\nError Category for Each System\\nAccuracy\\nFluency\\nFigure 1: Upper: Error severity for each system.\\nThe gray line represents the standard deviation for\\neach system across tasks. Bottom: Error category\\nanalysis for each system.\\nlations, which accounts for most of the disparity.\\nInterestingly, GPT-4 surpasses junior translators\\nin fluency issues, denoting a better capability of\\nlanguage usage.\\nIn addition, Figure 2 shows the top 5 categories\\nof errors made by different systems. ‘Mistransla-\\ntion’ is the most frequent error made by all systems.\\nImproving much over the seamless baseline, GPT-\\n4 makes comparable numbers of ‘Mistranslation’\\nwith junior and medium human translators.\\nFor all translators, ‘Unnatural Flow’ is among\\nthe most frequent errors. Seamless, GPT-4, and\\njunior translators have similar levels of ‘Unnatural\\nFlow’, indicating possible issues of literal transla-\\ntion and not following language conventions. In\\ncontrast, medium and senior translators are anno-\\ntated with significantly fewer errors of ‘Unnatural\\nFlow’.\\nIn addition, we notice even though GPT-4 makes\\nMistranslation\\nPunctuation\\nWrong NE\\nUnnatural Flow\\nOmission\\nMistranslationUnnatural Flow\\nWrong NEGrammar\\nPunctuation\\nMistranslationUnnatural Flow\\nGrammarWrong NE\\nPunctuation\\nMistranslationUnnatural Flow\\nWrong NEOmissionGrammar\\nMistranslationUnnatural Flow\\nAddition\\nPunctuation\\nGrammar\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80Number of Errors\\nT op5 Error Categories\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\nFigure 2: Top 5 categories of errors made by each\\nsystem.\\nmuch fewer ‘Wrong Name Entity(NE)’ errors com-\\npared to Seamless, which could be beneficial be-\\ncause of its huge knowledge acquired in the pre-\\ntraining stage, it still has a gap compared to human\\ntranslators.\\nFinally, we notice that GPT-4 does not have\\nOmission or Addition problems in its top-5 errors,\\nwhereas even senior translators have Addition er-\\nrors.\\n5.2 Detailed Results for Each Language\\nIn Figure 3, we present detailed results for each\\nlanguage pair, averaged over two directions.\\nEnglish-Chinese From Figure 3(a), GPT-4\\nshows the great capability of translating English\\nto Chinese and vice versa. From the radar chart,\\nwe can see that GPT-4 makes almost the same or\\nslightly fewer semantic errors (Omission, Addition,\\nand Mistranslation errors) than Junior and Senior\\ntranslators. Especially mistranslation errors, which\\nare generally considered most semantically detri-\\nmental, are better than junior and senior translators.\\nFor omission and addition errors, GPT-4 reaches al-\\nmost the same level as senior translators. However,\\nGPT-4 made significantly more lexical, stylistic,\\nand grammatical errors than human translators do.\\nThe error distribution of translation of GPT-4 meets\\nour expectations, as in the absence of reference,\\nGPT-4 will translate unfamiliar words directly and\\nliterally instead of seeking online materials or other\\nforms of help like human translators. Furthermore,\\ndue to the complexity and variability of Chinese,\\nthe translation of entity names or proper nouns is\\nusually not one-to-one, two above reasons together'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='Mistranslation(130)\\nAddition(4)\\nMT Hallucination(14)\\nOmission(29)\\nUntranslated(0)\\nWrong Name Entity & T erm(78)\\nGrammar(9)\\nPunctuation(11)\\nSpelling(4)\\nRegister(8)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(a) Chinese↔English\\nMistranslation(20)\\nAddition(8)\\nMT Hallucination(0)\\nOmission(17)\\nUntranslated(2)\\nWrong Name Entity & T erm(1)\\nGrammar(5)\\nPunctuation(3)\\nSpelling(3)\\nRegister(0)\\nInconsistent Style(3)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior (b) English↔Russian\\nMistranslation(52)\\nAddition(6)\\nMT Hallucination(5)\\nOmission(9)\\nUntranslated(6)\\nWrong Name Entity & T erm(7)\\nGrammar(6)\\nPunctuation(10)\\nSpelling(3)\\nRegister(1)\\nInconsistent Style(1)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\nmedium\\nsenior (c) Chinese↔Hindi\\nFigure 3: Error category results for each language. Each sub-figure is the average over two directions.\\nWe only include ‘Major’ errors here to highlight the most severe problems. Higher values indicate more\\nerrors and the number after each error type is the maximum number of that error.\\ncause the inferiority of the performance of GPT-4\\nin these aspects.\\nEnglish-Russian For the English-Russian trans-\\nlation tasks, GPT-4 made slightly more semantic\\nerrors but the number of mistranslation errors made\\nby GPT-4 is almost at the same level as medium\\nand senior translators. However, GPT-4 generally\\nmade less stylistic, grammatical, and wrong name\\nentity & term than junior translators. The English-\\nRussian translation tasks are quite challenging and\\nthe performance of translators varies significantly,\\nbut GPT-4 still maintains the average level overall.\\nHindi-Chinese As the low-resource language\\npair we evaluate, GPT-4 demonstrates the worst\\nperformance across evaluated translators. We ob-\\nserve that GPT4 is inferior to our MT baseline. This\\nmay be due to the small portion of Hindi and Chi-\\nnese corpora in its pre-training dataset. Specifically,\\nmaking the most ‘Mistranslation’ errors of GPT-4\\nindicates a distance away from the language under-\\nstanding of human translators. As a comparison,\\nSeamlessM4T performs better in both semantic and\\nlexical errors.\\nDiscussion Our results here manifest an imbal-\\nance of multilinguality for LLMs (Wang et al.,\\n2023b). Our results imply that GPT-4 can serve\\nas a reliable translator for resource-high such as\\nChinese to English but is doubtful for low-resource\\ndirections like Chinese-Hindi. In the low-resource\\nscenario, machine translator is more reliable.\\n5.3 Detailed Results for Different Domains\\nFigure 4 presents our results for different domains\\nin Chinese-to-English translation. We compare\\nthree different domains, including news, technol-\\nogy, and biomedical.\\nGeneral News Domain GPT-4 performs worse\\nin the news domain than human translators of three\\nlevels. The number of semantic errors made by\\nGPT-4 is quite close to junior and medium transla-\\ntors. Nonetheless, GPT-4 made more lexical and\\ngrammatical errors compared to human translators.\\nWe hypothesize the reasons for the situation de-\\nscribed above to happen are mainly because of the\\nliterariness and timeliness. Because GPT-4 is not\\nable to access the online materials to confirm the\\nname of a specific entity or event.\\nTechnology Domain The performance of GPT-4\\nis relatively close to medium-level translators. Ex-\\ncept for the Wrong Name Entity & Terms, GPT-4\\nmakes almost the same or even fewer errors than\\nmedium-level translators across all aspects. Specif-\\nically, the number of semantic errors made by GPT-\\n4 is almost the same to medium-level translators\\nand it makes much fewer structural and grammati-\\ncal errors. It means that in this field, GPT-4 might\\nunderstand the original text better than junior or\\nmedium-level translators and be able to conduct\\na translation that is more in line with the original\\nmeaning.\\nBiomedical Domain Similar to the technology\\ndomain, the qualities of the translations made by\\nGPT-4 and medium-level translators stand at the'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='Mistranslation(105)\\nAddition(4)\\nMT Hallucination(2)\\nOmission(27)\\nUntranslated(0)\\nWrong Name Entity & T erm(75)\\nGrammar(9)\\nPunctuation(7)\\nSpelling(2)\\nRegister(6)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(a) General news domain.\\nMistranslation(114)\\nAddition(3)\\nMT Hallucination(2)\\nOmission(57)\\nUntranslated(0)\\nWrong Name Entity & T erm(157)\\nGrammar(30)\\nPunctuation(9)\\nSpelling(3)\\nRegister(8)\\nInconsistent Style(5)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior (b) Technology domain.\\nMistranslation(21)\\nAddition(0)\\nMT Hallucination(3)\\nOmission(9)\\nUntranslated(0)\\nWrong Name Entity & T erm(49)\\nGrammar(0)\\nPunctuation(2)\\nSpelling(1)\\nRegister(0)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior (c) Biomedical domain.\\nFigure 4: Error category results for different domains in Chinese-to-English. We only include ‘Major’\\nerrors here to highlight the most severe problems. Higher values indicate more errors and the number in\\nthe bracket is the maximum number of that error.\\nSource 巨人网络有限公司\\nGPT-4 Giant Network Group Inc.\\nHuman Giant Interactive Group Inc.\\nTable 5: Named Entity cases.\\nsame level. Despite slightly more Wrong Name\\nEntity & Terms errors made, GPT-4 performs better\\nthan junior and medium-level translators in other\\naspects.\\nDiscussion For specific domains like technol-\\nogy, we show that GPT-4 is comparable with ju-\\nnior/medium translators. We still notice a similar\\nimbalance issue as in the multilingual setting, but\\nGPT-4’s performance is not as sensitive as in the\\nchange of language.\\n5.4 Case Study\\nWe also qualitatively understand the difference be-\\ntween the translations given by GPT-4 and human\\ntranslators.\\nLiteral Translations Among the error cases, the\\ntypical one is literal translations. Specifically, we\\nfind that GPT-4 sometimes translates with semanti-\\ncally correct, but in-native and literal translations.\\nThis is problematic with named entities, especially\\nthose occurring less frequently. As shown in Table\\n5, when not knowing the correct translation of ‘巨\\n人网络有限公司’, GPT-4 translates the term word\\nby word. However, the issue of name entities oc-\\ncurs less for human translators, partially because\\nthey would google it to find the correct translation.\\nSource It’s just a white screen or it times out load-\\ning it, or the page becomes unresponsive!\\nGPT-4\\n它只是一个白屏，要么是加载时超时，\\n要么页面变得无响应了！\\nHuman\\n页面要么显示空白，要么加载超时或是\\n无响应。\\nTable 6: Unnatural-Flow cases. Red represents the\\nliteral translation and green is more natural and\\nnative in Chinese.\\nThus, this issue might be resolved by incorporating\\nweb-search into agent-like translation (Feng et al.,\\n2024; Wu et al., 2024c).\\nExcept for named entities, we notice that the\\nliteral translation causes Unnatural Flows. As\\nshown in Table 6, when translating ‘It’s just a white\\nscreen’, GPT-4 translates the phrase to ‘ 它(it)只\\n是(is just)一个(a)白屏(white screen)’, but human\\ntranslator translates this phrase to ‘‘ 页面显示空\\n白(The page display is white)’’, which represents\\na preciser meaning and follows local conventions.\\nHuman Imagination We find human translators\\nalso have drawbacks compared to the GPT-4 trans-\\nlator. When the source sentence contains insuffi-\\ncient information to translate, human translators\\ntend to fill the gap by imagination or overthinking.\\nAn example is given in Table 7. The translator\\nwrongly understands the phrase ‘entering his 2nd\\nyear’ as Daley is a two-year-old baby, but the sen-\\ntence describes a 2nd-year player for sports. This\\nmay be due to daily language habits, misunder-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='Source He has health concerns atm but\\nwe also have Daley entering his 2nd year\\nand is a decent safety net.\\nGPT-4 他目前有健康问题，但我们还有戴利进\\n入他的第二年，他是一个不错的安全保\\n障。\\nHuman 他目前有健康问题。不过，戴利两岁\\n了，是个不错的备选人。\\nTable 7: Human imagination cases. Red denotes the\\nimagined part.\\nstanding, or not paying attention, and could be\\nrelated to the hallucination (Zhang et al., 2023) of\\nLLMs. GPT-4’s literal translation helps in this, as\\nit keeps faithful to the source sentence. This also\\naligns with our findings in Section 5.1 that GPT-4\\nhas fewer Additions or Omissions.\\n6 Conclusion\\nIn this study, we comprehensively evaluated the\\ntranslation quality of GPT-4 against human trans-\\nlators of varying expertise levels across multiple\\nlanguage pairs and domains. Our findings showed\\nthat GPT-4 performs comparably to junior transla-\\ntors in terms of total errors made but lags behind\\nmedium and senior translators. We also notice that\\nGPT-4’s translation capability gradually weakens\\nfrom resource-rich to resource-poor language pairs.\\nQualitative analysis revealed that GPT-4 tends to\\nproduce more literal translations compared to hu-\\nman translators but suffers less from imagined in-\\nformation.\\nThe results of this study demonstrate that GPT-4\\nhas made significant strides in approaching human-\\nlevel translation quality, as well as highlighting the\\nnuanced difference between them. This suggests\\npromising opportunities for collaboration and en-\\nhancement of translation workflows. As research\\ncontinues to advance, we anticipate that LLMs will\\nbecome increasingly valuable tools in the trans-\\nlation industry, working alongside human transla-\\ntors to improve productivity, efficiency, and overall\\ntranslation quality.\\n7 Limitations\\nOur work is limited in the following aspects: (1)\\nWe benchmark GPT-4 for translation tasks, as it is\\na representative large language model and shows\\nstate-of-the-art performance for many text-based\\ntasks. However, our evaluations can be extended\\nto other LLMs such as Claude-3. (2) Our eval-\\nuation covers three languages and six directions\\nfrom resource-rich to resource-poor. However, for\\nother languages, there might be linguistic-specific\\nphenomena that are not covered in this paper.\\nReferences\\nGuangsheng Bao, Yanbin Zhao, Zhiyang Teng,\\nLinyi Yang, and Yue Zhang. 2023. Fast-\\ndetectgpt: Efficient zero-shot detection of\\nmachine-generated text via conditional probabil-\\nity curvature. arXiv preprint arXiv:2310.05130.\\nLjubisa Bojic, Predrag Kovacevic, and Milan\\nCabarkapa. 2023. Gpt-4 surpassing human\\nperformance in linguistic pragmatics. arXiv\\npreprint arXiv:2312.09545.\\nJacob Cohen. 1960. A coefficient of agreement for\\nnominal scales. Educational and psychological\\nmeasurement, 20(1):37–46.\\nSeamless Communication, Loïc Barrault, Yu-An\\nChung, Mariano Cora Meglioli, David Dale,\\nNing Dong, Paul-Ambroise Duquenne, Hady\\nElsahar, Hongyu Gong, Kevin Heffernan, John\\nHoffman, Christopher Klaiber, Pengwei Li,\\nDaniel Licht, Jean Maillard, Alice Rakotoari-\\nson, Kaushik Ram Sadagopan, Guillaume Wen-\\nzek, Ethan Ye, Bapi Akula, Peng-Jen Chen,\\nNaji El Hachem, Brian Ellis, Gabriel Mejia\\nGonzalez, Justin Haaheim, Prangthip Hansanti,\\nRuss Howes, Bernie Huang, Min-Jae Hwang,\\nHirofumi Inaguma, Somya Jain, Elahe Kalbassi,\\nAmanda Kallet, Ilia Kulikov, Janice Lam, Daniel\\nLi, Xutai Ma, Ruslan Mavlyutov, Benjamin Pelo-\\nquin, Mohamed Ramadan, Abinesh Ramakrish-\\nnan, Anna Sun, Kevin Tran, Tuan Tran, Igor\\nTufanov, Vish V ogeti, Carleigh Wood, Yilin\\nYang, Bokai Yu, Pierre Andrews, Can Balioglu,\\nMarta R. Costa-jussà, Onur Celebi, Maha El-\\nbayad, Cynthia Gao, Francisco Guzmán, Justine\\nKao, Ann Lee, Alexandre Mourachko, Juan Pino,\\nSravya Popuri, Christophe Ropers, Safiyyah\\nSaleem, Holger Schwenk, Paden Tomasello,\\nChanghan Wang, Jeff Wang, and Skyler Wang.\\n2023. Seamlessm4t: Massively multilingual &\\nmultimodal machine translation.\\nMaxim Enis and Mark Hopkins. 2024. From\\nllm to nmt: Advancing low-resource ma-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='chine translation with claude. arXiv preprint\\narXiv:2404.13813.\\nAkhbardeh Farhad, Arkhangorodsky Arkady,\\nBiesialska Magdalena, Bojar Ondˇrej, Chatterjee\\nRajen, Chaudhary Vishrav, Marta R Costa-jussa,\\nEspaña-Bonet Cristina, Fan Angela, Federmann\\nChristian, et al. 2021. Findings of the 2021 con-\\nference on machine translation (wmt21). In Pro-\\nceedings of the Sixth Conference on Machine\\nTranslation, pages 1–88. Association for Com-\\nputational Linguistics.\\nZhaopeng Feng, Yan Zhang, Hao Li, Bei Wu, Jiayu\\nLiao, Wenqiang Liu, Jun Lang, Yang Feng, Jian\\nWu, and Zuozhu Liu. 2024. Tear: Improving\\nllm-based machine translation with systematic\\nself-refinement.\\nLukas Fischer and Samuel L¨\"aubli. 2020. What’s\\nthe difference between professional human and\\nmachine translation? a blind multi-language\\nstudy on domain-specific MT. In Proceedings\\nof the 22nd Annual Conference of the European\\nAssociation for Machine Translation, pages 215–\\n224, Lisboa, Portugal. European Association for\\nMachine Translation.\\nMarkus Freitag, George Foster, David Grang-\\nier, Viresh Ratnakar, Qijun Tan, and Wolfgang\\nMacherey. 2021. Experts, errors, and context:\\nA large-scale study of human evaluation for ma-\\nchine translation. Transactions of the Associ-\\nation for Computational Linguistics , 9:1460–\\n1474.\\nMarkus Freitag, Nitika Mathur, Chi-kiu Lo, Eleft-\\nherios Avramidis, Ricardo Rei, Brian Thompson,\\nTom Kocmi, Frederic Blain, Daniel Deutsch,\\nCraig Stewart, Chrysoula Zerva, Sheila Castilho,\\nAlon Lavie, and George Foster. 2023. Results\\nof WMT23 metrics shared task: Metrics might\\nbe guilty but references are not innocent. In Pro-\\nceedings of the Eighth Conference on Machine\\nTranslation, pages 578–628, Singapore. Associ-\\nation for Computational Linguistics.\\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-\\nkiu Lo, Craig Stewart, Eleftherios Avramidis,\\nTom Kocmi, George Foster, Alon Lavie, and\\nAndré F. T. Martins. 2022. Results of WMT22\\nmetrics shared task: Stop using BLEU – neu-\\nral metrics are better and more robust. In Pro-\\nceedings of the Seventh Conference on Machine\\nTranslation (WMT), pages 46–68, Abu Dhabi,\\nUnited Arab Emirates (Hybrid). Association for\\nComputational Linguistics.\\nTanya Goyal, Junyi Jessy Li, and Greg Durrett.\\n2022. News summarization and evaluation in the\\nera of gpt-3. arXiv preprint arXiv:2209.12356.\\nYvette Graham, Timothy Baldwin, Alistair Mof-\\nfat, and Justin Zobel. 2013. Continuous mea-\\nsurement scales in human evaluation of machine\\ntranslation. In Proceedings of the 7th Linguis-\\ntic Annotation Workshop and Interoperability\\nwith Discourse, pages 33–41, Sofia, Bulgaria.\\nAssociation for Computational Linguistics.\\nYvette Graham, Christian Federmann, Maria Es-\\nkevich, and Barry Haddow. 2020. Assessing\\nhuman-parity in machine translation on the seg-\\nment level. In Findings of the Association for\\nComputational Linguistics: EMNLP 2020, pages\\n4199–4207, Online. Association for Computa-\\ntional Linguistics.\\nTianyu Han, Lisa C Adams, Keno Bressem, Fe-\\nlix Busch, Luisa Huck, Sven Nebelung, and\\nDaniel Truhn. 2023. Comparative analysis of\\ngpt-4vision, gpt-4 and open source llms in clin-\\nical diagnostic accuracy: A benchmark against\\nhuman expertise. medRxiv, pages 2023–11.\\nHany Hassan, Anthony Aue, Chang Chen, Vishal\\nChowdhary, Jonathan Clark, Christian Fed-\\nermann, Xuedong Huang, Marcin Junczys-\\nDowmunt, William Lewis, Mu Li, et al. 2018.\\nAchieving human parity on automatic chinese\\nto english news translation. arXiv preprint\\narXiv:1803.05567.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy\\nZou, Mantas Mazeika, Dawn Song, and Jacob\\nSteinhardt. 2021. Measuring massive multitask\\nlanguage understanding.\\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\\nVikas Raunak, Mohamed Gabr, Hitokazu Mat-\\nsushita, Young Jin Kim, Mohamed Afify, and\\nHany Hassan Awadalla. 2023. How good are gpt\\nmodels at machine translation? a comprehensive\\nevaluation. arXiv preprint arXiv:2302.09210.\\nHui Huang, Shuangzhi Wu, Xinnian Liang, Bing\\nWang, Yanrui Shi, Peihao Wu, Muyun Yang, and\\nTiejun Zhao. 2023. Towards making the most of'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='llm for translation quality estimation. In CCF\\nInternational Conference on Natural Language\\nProcessing and Chinese Computing, pages 375–\\n386. Springer.\\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang,\\nXing Wang, Shuming Shi, and Zhaopeng Tu.\\n2023a. Is chatgpt a good translator? yes with\\ngpt-4 as the engine.\\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang,\\nXing Wang, Shuming Shi, and Zhaopeng Tu.\\n2023b. Is chatgpt a good translator? yes with\\ngpt-4 as the engine.\\nAhrii Kim, Yunju Bak, Jimin Sun, Sungwon Lyu,\\nand Changmin Lee. 2023. The suboptimal\\nwmt test sets and its impact on human parity.\\nPreprints.\\nFilip Klubi ˇcka, Antonio Toral, and Víctor M\\nSánchez-Cartagena. 2018. Quantitative fine-\\ngrained human evaluation of machine transla-\\ntion systems: a case study on english to croatian.\\nMachine Translation, 32(3):195–215.\\nTom Kocmi, Eleftherios Avramidis, Rachel Baw-\\nden, Ond ˇrej Bojar, Anton Dvorkovich, Chris-\\ntian Federmann, Mark Fishel, Markus Freitag,\\nThamme Gowda, Roman Grundkiewicz, et al.\\n2023. Findings of the 2023 conference on ma-\\nchine translation (wmt23): Llms are here but\\nnot quite there yet. In Proceedings of the Eighth\\nConference on Machine Translation, pages 1–42.\\nTom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton\\nDvorkovich, Christian Federmann, Mark Fishel,\\nThamme Gowda, Yvette Graham, Roman Grund-\\nkiewicz, Barry Haddow, et al. 2022. Findings\\nof the 2022 conference on machine translation\\n(wmt22). In Proceedings of the Seventh Con-\\nference on Machine Translation (WMT), pages\\n1–45.\\nKlaus Krippendorff. 1980. Validity in content anal-\\nysis. Computerstrategien für die Kommunika-\\ntionsanalyse, 69:45p.\\nYafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue\\nWang, Linyi Yang, Shuming Shi, and Yue Zhang.\\n2023. Deepfake text detection in the wild. arXiv\\npreprint arXiv:2305.13242.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\\nJiang, Hiroaki Hayashi, and Graham Neubig.\\n2023a. Pre-train, prompt, and predict: A sys-\\ntematic survey of prompting methods in natural\\nlanguage processing. ACM Computing Surveys,\\n55(9):1–35.\\nZhengliang Liu, Tianyang Zhong, Yiwei Li, Yu-\\ntong Zhang, Yi Pan, Zihao Zhao, Peixin Dong,\\nChao Cao, Yuxiao Liu, Peng Shu, et al. 2023b.\\nEvaluating large language models for radiology\\nnatural language processing. arXiv preprint\\narXiv:2307.13693.\\nArle Lommel, Maja Popovic, and Aljoscha Bur-\\nchardt. 2014. Assessing inter-annotator agree-\\nment for translation error annotation. In MTE:\\nWorkshop on Automatic and Manual Metrics for\\nOperational Translation Evaluation, pages 31–\\n37. Language Resources and Evaluation Confer-\\nence Reykjavik.\\nLaurence T Maloney, Maria F Dal Martello, Vi-\\nvian Fei, and Valerie Ma. 2024. A compar-\\nison of human and gpt-4 use of probabilistic\\nphrases in a coordination game. Scientific re-\\nports, 14(1):6835.\\nHiroki Nakayama, Takahiro Kubo, Junya Ka-\\nmura, Yasufumi Taniguchi, and Xu Liang.\\n2018. doccano: Text annotation tool\\nfor human. Software available from\\nhttps://github.com/doccano/doccano.\\nHa Nguyen and Vicki Allan. 2024. Using gpt-4\\nto provide tiered, formative code feedback. In\\nProceedings of the 55th ACM Technical Sympo-\\nsium on Computer Science Education V . 1, pages\\n958–964.\\nKeqin Peng, Liang Ding, Qihuang Zhong, Li Shen,\\nXuebo Liu, Min Zhang, Yuanxin Ouyang, and\\nDacheng Tao. 2023. Towards making the most\\nof chatgpt for machine translation. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2023, pages 5622–5633.\\nThierry Poibeau. 2022. On\" human parity\" and\" su-\\nper human performance\" in machine translation\\nevaluation. In Language Resource and Evalua-\\ntion Conference.\\nRicardo Rei, Craig Stewart, Ana C Farinha, and\\nAlon Lavie. 2020a. COMET: A neural frame-\\nwork for MT evaluation. In Proceedings of the'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='2020 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP), pages 2685–\\n2702, Online. Association for Computational\\nLinguistics.\\nRicardo Rei, Craig Stewart, Ana C Farinha, and\\nAlon Lavie. 2020b. Comet: A neural frame-\\nwork for mt evaluation. In Proceedings of the\\n2020 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP), pages 2685–\\n2702.\\nSai Cheong Siu. 2023. Chatgpt and gpt-4 for pro-\\nfessional translators: Exploring the potential of\\nlarge language models in translation. Available\\nat SSRN 4448091.\\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy\\nWay. 2018. Attaining the unattainable? reassess-\\ning claims of human parity in neural machine\\ntranslation. In Proceedings of the Third Confer-\\nence on Machine Translation: Research Papers,\\npages 113–123, Brussels, Belgium. Association\\nfor Computational Linguistics.\\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui\\nZhang, Dian Yu, Shuming Shi, and Zhaopeng\\nTu. 2023a. Document-level machine translation\\nwith large language models.\\nWenxuan Wang, Wenxiang Jiao, Jingyuan Huang,\\nRuyi Dai, Jen-tse Huang, Zhaopeng Tu, and\\nMichael R Lyu. 2023b. Not all countries cel-\\nebrate thanksgiving: On the cultural dominance\\nin large language models. CoRR.\\nMinghao Wu, Thuy-Trang Vu, Lizhen Qu, George\\nFoster, and Gholamreza Haffari. 2024a. Adapt-\\ning large language models for document-\\nlevel machine translation. arXiv preprint\\narXiv:2401.06468.\\nMinghao Wu, Yulin Yuan, Gholamreza Haffari,\\nand Longyue Wang. 2024b. (perhaps) beyond\\nhuman translation: Harnessing multi-agent col-\\nlaboration for translating ultra-long literary texts.\\narXiv preprint arXiv:2405.11804.\\nMinghao Wu, Yulin Yuan, Gholamreza Haffari,\\nand Longyue Wang. 2024c. (perhaps) beyond\\nhuman translation: Harnessing multi-agent col-\\nlaboration for translating ultra-long literary texts.\\nHaoran Xu, Young Jin Kim, Amr Sharaf, and\\nHany Hassan Awadalla. 2023. A paradigm shift\\nin machine translation: Boosting translation per-\\nformance of large language models.\\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chen-\\njie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian\\nYu, Cong Yu, et al. 2020. Clue: A chinese\\nlanguage understanding evaluation benchmark.\\narXiv preprint arXiv:2004.05986.\\nFanghua Ye, Mingming Yang, Jianhui Pang,\\nLongyue Wang, Derek F. Wong, Emine Yilmaz,\\nShuming Shi, and Zhaopeng Tu. 2024. Bench-\\nmarking llms via uncertainty quantification.\\nLifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng\\nGao, Fangyuan Zou, Xingyi Cheng, Heng Ji,\\nZhiyuan Liu, and Maosong Sun. 2023. Revisit-\\ning out-of-distribution robustness in nlp: Bench-\\nmark, analysis, and llms evaluations.\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao\\nLiu, Tingchen Fu, Xinting Huang, Enbo Zhao,\\nYu Zhang, Yulong Chen, et al. 2023. Siren’s\\nsong in the ai ocean: a survey on hallucina-\\ntion in large language models. arXiv preprint\\narXiv:2309.01219.\\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein,\\nand Sameer Singh. 2021. Calibrate before use:\\nImproving few-shot performance of language\\nmodels. In International conference on machine\\nlearning, pages 12697–12706. PMLR.\\nJie Zhu, Junhui Li, Yalong Wen, and Lifan Guo.\\n2024. Benchmarking large language models\\non cflue–a chinese financial language under-\\nstanding evaluation dataset. arXiv preprint\\narXiv:2405.10542.\\nA Expertise of Human Annotators\\nTo categorize translators into junior, medium, or\\nsenior levels, we have established a comprehensive\\nset of criteria that take into account various factors\\nindicative of a translator’s expertise and experience.\\nThese factors include the translator’s educational\\nbackground, particularly the prestige of the insti-\\ntution from which they graduated, as well as their\\nlength of service in the translation industry, the\\nduration of their translation career, the number of\\ntranslations completed, and any professional certifi-\\ncations they have obtained. To ensure the ongoing\\ncompetence of our translators, we conduct quar-\\nterly assessments to evaluate their performance.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='For instance, to be classified as a senior-level trans-\\nlator, an individual must possess a minimum of\\nten years of translation experience, demonstrate\\nexceptional proficiency by achieving a score of\\n99% on our assessments, and hold the distinguished\\nCATTI++ translation certification. By considering\\nthese stringent criteria, we aim to maintain a highly\\nqualified and skilled pool of translators across all\\nlevels of expertise.\\nB Annotation Requirements\\nB.1 Error Types\\nOur annotation system is built upon the open-\\nsourced doccano system 3. In Figure 5, we provide\\na screenshot of our annotation system. For each\\nsource sentence, outputs for different systems are\\ngiven and the annotators can select spans of the text\\nand annotate the error type and severity.\\nC Detailed Explanation and Guidance for\\nEach Error Types\\nOur evaluation protocol largely follows the MQM\\ncriteria released by Unbabel 4. We provide a de-\\ntailed annotation manual for annotators, including\\nan explanation for each error type as well as illus-\\ntrative examples for error types. It is included in\\nthe following:\\nC.1 Annotation Requirements\\nThe minimum unit that can be selected and anno-\\ntated is a whole word, a whitespace, a punctuation\\nmark, or an isolated character. In the following\\nexample, the version in French has an extra excla-\\nmation mark, so it’s necessary to annotate it as a\\nPunctuation error:\\n[EN] Thank you very much.\\n[FR] Merci beaucoup!\\nWrong selection →Merci [beau-\\ncoup!]PUNCTUATION\\nCorrect selection →Merci beau-\\ncoup[!]PUNCTUATION\\nIf the issue occurs in a multiword expression,\\nyou will need to select the whole expression; if, for\\nexample, an entire sentence was translated and it\\nshouldn’t have been, you should select the entire\\nsentence.\\n3https://github.com/doccano/doccano\\n4\\nIn the following example, we have an Unnatural\\nFlow error:\\n[EN] Hi, Mary here.\\n[ES] Hola, Mary aquí.\\nWrong selection → Hola, [Mary\\naquí.]UNNATURAL FLOW\\nCorrect selection → Hola, [Mary\\naquí]UNNATURAL FLOW.\\nC.2 Error Types\\nAccuracy\\n• Mistranslation\\n– Description: Translation does not accu-\\nrately represent the source.\\n– Example:\\n[EN] It has to be done by the book.\\n[FR] Il doit être fait [par le\\nlivre]MISTRANSLATION\\n[Reason] The word-for-word trans-\\nlation into French doesn’t work.\\n• Addition\\n– Description: Information not present in\\nthe source.\\n– Example:\\n[EN] That way you can be sure that\\nyou were the one who made the\\nchanges.\\n[ES] Así puedes estar seguro de que\\nfuiste tú quien hizo [todos ADDI-\\ntIoN los cambios.\\n[Reason] [Todos] (meaning ’all’ in\\nSpanish) is not present in the source\\nand it is incorrectly added in the\\ntarget text.\\n• MT Hallucination\\n– Description: information that has noth-\\ning related to source; or gibberish; or\\nrepeats\\n– Example:\\n[EN] You can send us a follow-up\\nemail at this address [EMAIL].\\n[ES] [Hágame saber si tiene al-\\nguna otra pregunta]MT HALLUCI-\\nNATION.]\\n[Reason]: The Spanish translation'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='Figure 5: A screenshot of the Doccano annotation system we use.\\nreads please let me know if you\\nhave any other questions and it’s\\ngrammatically correct and fluent,\\nbut it has no relation at all with the\\nsource.]\\n• Omission\\n– Description: Missing content from the\\nsource.\\n– Example:\\n[EN] We do not have much informa-\\ntion on this.\\n[FR] Nous ne disposons\\npas [] OMISSION beaucoup\\nd’informations à ce sujet.\\n[Reason]: The French sentence\\nrequires the preposition [de] (dis-\\nposer de).\\n• Untranslated\\n– Description: Not translated.\\n– Example:\\n[EN] How To Make Pizza Dough\\n[FR] Comment faire de [Pizza\\nDough|UNTRANSLATED\\n[Reason]: [Pizza Dough] is not a\\nnamed entity and is untranslated in\\nthe French version.\\n• Wrong Name Entity & Term\\n– Description: Wrong usage of NE and\\nTerminology.\\n– Example:\\n[EN] Dear Wiley,\\n[IT] Gentile [Wilar WRONG\\nNAMED ENTITY,\\n[Reason]: The name in the Italian\\nversion doesn’t match the original.\\nFluency\\n• Grammar\\n– Description: Problems with grammar of\\ntarget language.\\n– Example:\\n[EN] I understand that you want to\\ncheck in online.\\n[CS] chàpu, ze se chcete\\n[odbavení]gRAMMaR online.\\n[Reason]: Wrong part of speech\\nmakes the sentence ungrammatical\\nin Czech.\\n• Punctuation\\n– Description: incorrect punctuation (for\\nlocale or style).\\n– Example:'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='[EN] Original copy of the Proof of\\nPurchase or Invoice (not a screen-\\nshot):\\n[PT] C’opia original do com-\\nprovante de compra ou nota\\nfiscal (não uma captura de\\ntela)[.]PUNCTUATION\\n[Reason]: There’s a period instead\\nof a colon in the Brazilian Por-\\ntuguese version of this sentence.\\n• Spelling\\n– Description: incorrect spelling or capital-\\nization.\\n– Example:\\n[EN] This sort of damage is not cov-\\nered under the warranty, but we will\\nseek assistance from a higher sup-\\nport and see what we can do regard-\\ning this issue.\\n[IT] Questo tipo di danno non è cop-\\nerto dalla garanzia, ma chiederò\\ncomunque aiuto ai responsabili\\ndell’assistenza per capire che cosa\\n[Zi]SPELLING può fare per quanto\\nriguarda questo problema.\\n[Reason]: There’s a typo in the\\nsentence in Italian: the word [zi]\\nshould be [si] instead.\\n• Register\\n– Description: Wrong grammatical regis-\\nter (e.g., inappropriately informal pro-\\nnouns).\\n– Example:\\n[EN] Wishing you a great day\\nahead.\\n[DE] Ich wünsche [Ih-\\nnen]REGISTER einen schönen\\nTag.\\n[Reason]: The required register for\\nthe German translation is Informal\\nbut the pronoun [Inhen] is Formal.\\n• Inconsistent Style\\n– Description: internal inconsistency (not\\nrelated to terminology).\\n– Example:\\n[EN] Please click on this link. [...]\\nThis link will expire in 24 hours.\\n[NN] Klikk på denne\\n[lenken].[...]Denne\\n[linken]INCONSISTENCY ut-\\nloper om 24 timer.\\n[Reason]: Both [lenk] and [link]\\nare correct in Norwegian, but in the\\nsame document, only one should be\\nused. Note: this is a single error,\\nnot two\\n• Unnatural Flow\\n– Description: translations that are too lit-\\neral or sound unnatural.\\n– Example:\\n[EN] Zebras are ideal for animal\\nmatching.\\n[DE] [Zebras sind ideal,\\num bestimmte Tiere zu\\nfinden]UNNATURAL FLOW.\\n[Reason] The German translation\\nsounds too literal, it reads like a\\ntranslation, using the verb [finden]\\n(finding) as a translation for match-\\ning. The verb matching should be\\ntranslated as [detektieren] (detect)\\nto read as if it was originally written\\nin the target language: [Zebras sind\\nein ideales Beispiel zur Detektion\\nvon Wildtieren.]\\nOther\\n• Non-translation\\nD Extra Details\\nD.1 Translation Prompt in Preliminary Study\\nIn two experiments, the translation prompt we use\\nis as follows:\\n• Please translate the following sentences from\\n<SRC_LANG> to <TGT_LANG>. Ensure\\nline alignment across the document while\\nmaintaining the fluency of overall translation.\\nThe prompt asks GPT4 to maintain the sentence\\nalignment of the given document, so each sentence\\ncan be aligned back to its source sentence while be-\\ning translated at the document level. In practice, we\\nfind most times GPT4 can follow our instructions.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-07-08T00:23:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-07-08T00:23:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GPT-4_VS_Human_translators.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='Occasionally, it fails to keep the sentence structure\\nof the document and merges some sentences in one\\nrow. In these cases, we manually split the merged\\nsentences.\\nD.2 Model and Decoding\\nFor GPT-4, we use greedy search for decoding,\\nto ensure the reproducibility of the results. For\\nSeamlessM4T, we use the 2.3B version of seam-\\nlessM4T_v2_large and adopt beam search with\\nbeam size 5.')]\n"
     ]
    }
   ],
   "source": [
    "DOC_PATH = os.getenv('DOC_PATH')\n",
    "print(DOC_PATH)\n",
    "doc = loader(DOC_PATH)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def split_text(documents : list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 600,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len,\n",
    "        add_start_index = True,\n",
    "        seperators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(\"Document Length : \",len(documents))\n",
    "    print(\"No.of Chunks : \",len(chunks))\n",
    "    print(chunks[0].metadata)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini-embedding-exp-03-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-shutil\n",
      "  Downloading pytest_shutil-1.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from pytest-shutil) (1.17.0)\n",
      "Collecting execnet (from pytest-shutil)\n",
      "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pytest (from pytest-shutil)\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting termcolor (from pytest-shutil)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in ./.venv/lib/python3.10/site-packages (from pytest->pytest-shutil) (1.2.2)\n",
      "Collecting iniconfig (from pytest->pytest-shutil)\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from pytest->pytest-shutil) (24.2)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->pytest-shutil)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: tomli>=1 in ./.venv/lib/python3.10/site-packages (from pytest->pytest-shutil) (2.2.1)\n",
      "Downloading pytest_shutil-1.8.1-py3-none-any.whl (15 kB)\n",
      "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: termcolor, pluggy, iniconfig, execnet, pytest, pytest-shutil\n",
      "Successfully installed execnet-2.1.1 iniconfig-2.1.0 pluggy-1.5.0 pytest-8.3.5 pytest-shutil-1.8.1 termcolor-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytest-shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path = os.getenv('CHROMA_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "def save(chunks : list[Document]):\n",
    "    db = Chroma.from_documents(chunks,\n",
    "                               GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",google_api_key = API_KEY),\n",
    "                               persist_directory=chroma_path)\n",
    "    db.persist()\n",
    "    print(\"Saved Chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_store():\n",
    "    documents = loader()\n",
    "    chunks = split_text(documents)\n",
    "    save(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_content = \"\"\"\n",
    "Role:\n",
    "    You are a document insights provider. Your task is to generate well-structured responses using tables, bullet points, \n",
    "    or well-formatted paragraphs when appropriate.\n",
    "\n",
    "Guidelines:\n",
    "    - Use only the provided document as the source of truth.\n",
    "    - Do not generate information beyond the given context (avoid hallucinations).\n",
    "    - Ensure responses are clear, structured, and directly relevant to the question.\n",
    "\n",
    "Input:\n",
    "    - Context: {context}\n",
    "    - Question: {question}\n",
    "\n",
    "Output Format:\n",
    "    Provide a response that is:\n",
    "        - Concise yet informative.\n",
    "        - Properly structured (e.g., tables, bullet points, or paragraphs based on content type).\n",
    "        - Strictly based on the given document.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "def query(question):\n",
    "    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",google_api_key = API_KEY)\n",
    "\n",
    "    db = Chroma(persist_directory=chroma_path,embedding_function=embedding_function)\n",
    "    results = db.similarity_search_with_relevance_scores(question,k=4)\n",
    "\n",
    "    if(len(results) == 0 or results[0][1] < 0.4):\n",
    "        print(\"Your question is not relevant to the given document\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n - - \\n\\n\".join([doc.page_content for doc, _store in results])\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt_content)\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "    answer = chain.invoke({'context':context_text,'question':question}).content\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18323/249265487.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=chroma_path,embedding_function=embedding_function)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your question is not relevant to the given document\n",
      "No response\n"
     ]
    }
   ],
   "source": [
    "question = 'Which test sets were used to sample the source sentences for the Chinese to English and English to Russian tasks?'\n",
    "result = query(question)\n",
    "if result :\n",
    "    response, source = result\n",
    "    print(\"Response : \",response)\n",
    "    print(\"Source : \",source)\n",
    "else:\n",
    "    print(\"No response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
